# 磁盘配额（Quota）与进阶文件系统管理

如果 LInux 服务器有多个用户经常存取数据时，为了维护所有用户在硬盘容量的公平使用，磁盘配额（Quota）是一个非常有用的工具。

如果用户经常抱怨磁盘容量不够用，更进阶的文件系统就需要学习

本章会介绍磁盘阵列（RAID）及逻辑滚动条文件系统（LVM），这些工具都可以帮助你管理与维护用户可用的磁盘容量

## 磁盘配额（Quota）的应用于实践

Quota 字面意思是「限额」，对于磁盘来说就是限制容量的使用

## 什么是 Quota？

就是限制用户对磁盘容量的使用，不至于让其中某一个用户使用了大量的硬盘容量，导致其他用户不够用的情况

### Quota 的一般用途

针对网络服务的设计，比较常用的情况是：

- 针对 www server，如：每个人的网页空间的容量限制
- 针对 mail server，如：每个人的邮件空间限制
- 针对 file server，如：每个人最大的可用网络磁盘空间（教学环境中最常见）

针对 Linux 系统主机上面的使用者有如下常用情况：

- 使用群组限制：限制某一群组所能使用的最大磁盘配额

- 使用用户限制：限制某一用户的最大磁盘配额

- 限制某一目录的最大磁盘配额

  在 EXT 家族文件系统的磁盘配额主要是针对整个文件系统来处理，所以大多针对挂载点进行设计。

  新的 xfs 可以使用 project 模式，可以针对个别的目录（非文件系统）来设计磁盘配额

基本上 qutoa 让管理员知道磁盘使用率以及管理磁盘使用情况的一个工具。比较特别的是 XFS 的 quota 是整合到文件系统内，并不是其他外挂程序来管理的。所以通过 quota 来直接回报磁盘使用率，要比 unix 工具快速。如 `du`会重新计算目录下的磁盘使用率，但 xfs 可以通过 xfs_quota 直接获得各个目录的使用率，速度要快很多

### Quota 的使用限制

- EXT 文件系统家族只能针对整个 filesystem，xfs 可以使用 project 模式来为不同目录磁盘配额

- 核心必须支持 quota

  Linux 核心必须有支持 quota 功能，CentOS7.x 预设支持 quota 功能

- 只对一般身份使用者有效

  root 不能设置 quota，因为整个服务器数据都是他的

- 若启用 SELinux，非所有目录均可设置 quota

  新版 CentOS 预设启用 SELinux 功能，该功能加强了某些细部的权限控制，所以预设情况下，只能针对 `/home/`进行配置 quota

  如果要针对其他目录配置 quota，后续章节会讲解怎么解开 SELinux 限制的方法

不同文件系统 quota 的处理不太一样，在 quota 前，先确认你的文件系统

### Quota 的规范设置项目

针对 XFS filesystem 的限制项目主要分为以下几部分：

- 分别针对用户 user、群组 group、个别目录 project

  quota 的限制中，主要针对以上项进行磁盘使用率的限制

- 容量限制 block 或文件数量 inode 限制

  第 7 章中说到，文件系统主要规划为存放属性的 inode 与实际文件数据的 block 区块。

  - 限制 inode 用量：限制可以建立的 文件数量
  - 限制 block 用量：管理用户磁盘容量限制，较常见该方式

- 柔性 soft 劝导与硬性 hard 规定

  inode 和 block 限制值有两个：soft 与 hard，通常 hard 值比 soft 高。

  如：限制项目为 block，可以限制 hard 为 500MBytes ，soft 为 400MBytes。

  - hard：表示使用者绝对不会超过该值。
  - soft：表示使用者在地狱 soft 限制时，可以正常使用磁盘，但若超过 soft 且低于 hard 值，每次用户登录系统时，系统会主动发出磁盘即将爆满的警告信息，且会给予一个宽限时间（grace time），如果在宽限时间内降低到了 soft 限制下，则宽限时间会停止

- 会倒计时的宽限时间（grace time）

  只有用户的磁盘用量介于 soft 到 hard 之间时，才会出现倒计时，由于达到 hard 值时，用户的磁盘使用权可能会被锁住。为了担心用户没有注意到磁盘配额问题，因此涉及了 soft，相当于预警机制，一般预设的宽限时间为 7 天，**如果 7 天内你还不进行容量降低动作，那么 soft 限制会立刻取代 hard 值来做为 quota 的限制**

## 一个 XFS 文件系统的 Quota 实验

一个场景来实践，如下是需求描述

- 目的与账户

  让 5 个专题生为一组，账户是 myquota1、myquota2、myquota3、myquota4、myquota5 ，密码都是 123456，初始群组都是 myquotagrp。其他属性都为默认值

- 账户的磁盘容量限制

  5 个用户都能使用 300MBytes 的磁盘使用量（hard），文件数量不限制。此外，只要容量使用超过 250MBytes，则警告（soft）

- 群组限额（option 1）

  由于该系统还有其他用户存在，因此 myquotagrp 群组最多可用使用 1GBytes 的容量。也就是说 myquota1~3 都用了 280Mbytes，那么剩下两人最多只能使用 1000MB - 280x3 = 160MB 的磁盘容量。这就是使用者与群组同时设置时会产生的后果

- 共享目录限额（option 2）

  另一种设置方式是：每个用户具有自己的独立容量上限，他们的专题共享目录在 `/home/myquota` 该目录设置为其他人没有任何权限的共享空间，只有 myquotagrp 群组拥有全部的权限。且该目录最多只能使用 500MB 的磁盘容量。**群组 group 的限制与目录 （directory/project）无法同时设置**

- 宽限时间限制：每个使用者在超过 soft 限制值之后，有 14 天的宽限时间

接下来一步一步实现以上需求

```bash
# 多账户创建，可以使用之前的批量账户创建脚本来实现，这里使用简化版的定制脚本来实现

[root@study ~]# vim addaccount2.sh
#!/bin/bash
# 使用脚本来创建 xfs 文件系统 Quota 实验的账户
groupadd myquotagrp
for username in myquota1 myquota2 myquota3 myquota4 myquota5 
do
	useradd -g myquotagrp $username
	echo "123456" | passwd --stdin $username
done

mkdir /home/myquota
chgrp myquotagrp /home/myquota		# 改变该目录的所属群
chmod 2770 /home/myquota		# 拥有者和群组有权限，这里加了个 SGID=2 的特殊权限，笔者暂时不理解是为什么


[root@study ~]# sh addaccount2.sh 
```

## 实验流程 1：文件系统的支持与观察

XFS 默认支持 Quota，Ext 则请自行百度。另外 **不要在根目录下进行 quota，因为文件系统会变得太复杂**，下面以 `/home` 这个 xfs 文件系统为例，如何启动 quota 功能

```bash
# 查看文件系统
# -h 格式化容量单位，-T 显示 type 字段
[root@study ~]# df -hT /home
Filesystem                    Type  Size  Used Avail Use% Mounted on
/dev/mapper/centos_study-root xfs    35G  4.4G   31G  13% /
# 可以看到上面的挂载点是 / 根目录，这里是由于该指令不是在最初我们搭建分区的哪台学习机上操作的
# 所以最好还是切换到我们的学习机上面去实验；目前由于在外面，暂时没有环境
```

在过去的版本中，管理员可以通过 `mount -o remount` 机制来重新挂载启动 quota 的功能，不过当前的 XFS 文件系统在挂载之初就声明了，因此无法使用 remount 来重新启动 quota 功能，一定要写入 `/etc/fstab` 中，或者是在初始挂载过程中加入这个项目，否则不会生效

```bash
[root@study ~]# vim /etc/fstab
/dev/mapper/centos_study-root   /    xfs     defaults,usrquota,grpquota        0 0
# 第一字端对应 df -hT /home 中出现的第一字段。这里只增加了第 4 字段的这部分内容 ,usrquota,grpquota

# 卸载根目录，肯定会提示忙碌；这里只有重启让自动挂载了
[root@study ~]# umount /       
umount: /: target is busy.
        (In some cases useful info about processes that use
         the device is found by lsof(8) or fuser(1))
# 这里尝试使用根目录来启用 quota 功能，配置，重启之后，也没有被开启，这里我放弃了
```

好了，回到了最开始学习分好区那台学习机上了，下面再来做这个实验

```bash
[root@study ~]$ df -hT /home
Filesystem              Type  Size  Used Avail Use% Mounted on
/dev/mapper/centos-home xfs   5.0G  128M  4.9G   3% /home

# 先看看该目录提供的服务
[root@study ~]$ mount | grep home
/dev/mapper/centos-home on /home type xfs (rw,relatime,seclabel,attr2,inode64,noquota)

# 增加挂载时提供的服务 ,usrquota,grpquota 
[root@study ~]# vim /etc/fstab
/dev/mapper/centos-home /home                   xfs     defaults,usrquota,grpquota        0 0
# 还是提示繁忙，这里只能用重启代替，下面的2个步骤
# 原因是有 home 下面的账户在登录，因此无法卸载掉
[root@study ~]# umount /home
umount: /home: target is busy.
        (In some cases useful info about processes that use
         the device is found by lsof(8) or fuser(1))
[root@study ~]# mount -a	# 挂载
# 仔细看下面的信息，证明已经启动了
[mrcode@study ~]$ mount | grep home
/dev/mapper/centos-home on /home type xfs (rw,relatime,seclabel,attr2,inode64,usrquota,grpquota)
```

基本个，针对 quota 限制的项目主要有三项：

- `uquota/usrquota/quota`：针对使用者账户的设置
- `gquota/grpquota`：针对群组的设置
- `pquota/prjquota`：针对单一目录的设置，不可与  grpquota 同时存在

务必小心 修改`/etc/fstab`文件内容，发生错误的话，有可能导致无法开机

## 实验流程 2：观察 Quota 报告资料

可以使用 xfs_quota 指令来查阅相关信息，该指令参数较多，先来了解简单查询的参数

```bash
xfs_quota -x -c "指令" [挂载点]

选项与参数：
	-x：专家模式，后续才能够加入 -c 的指令参数
	-c：后面接指令，本小节先讲解数据回报的指令
指令：
	print：单纯的列出目前主机内的文件系统参数等信息
	df：与原本的  df 功能一样，可以加上 -b(block)  -i(inode) -h（格式化单位）等
	report：列出目前的 quota 项目，有 -ugr（user/group/project） 和 -bi 等信息
	state：列出目前支持 quota 的文件系统信息，有无启动相关项目等信息
```

```bash
# 范例 1：列出目前系统的各个文件系统，以及文件系统的 quota 挂载参数支持
[root@study ~]$ xfs_quota -x -c "print"
Filesystem          Pathname
/                   /dev/mapper/centos-root
/boot               /dev/sda2
/home               /dev/mapper/centos-home (uquota, gquota)		# 这里显示支持

# 范例 2：列出目前 /home 支持 quota 的挂载点文件系统使用情况
[root@study ~]$ xfs_quota -x -c "df -h" /home
Filesystem     Size   Used  Avail Use% Pathname
/dev/mapper/centos-home
               5.0G 127.7M   4.9G   2% /home
# 和 df -h 类似，只是更准确了 
```

```bash
# 范例 3： 列出目前 /home 的所有用户的  quota 限制值
[root@study ~]# xfs_quota -x -c "report -ubih" /home
User quota on /home (/dev/mapper/centos-home)
                        Blocks                            Inodes              
User ID      Used   Soft   Hard Warn/Grace     Used   Soft   Hard Warn/Grace  
---------- --------------------------------- --------------------------------- 
root            0      0      0  00 [------]      8      0      0  00 [------]
mrcode       7.4M      0      0  00 [------]    260      0      0  00 [------]
mrcode1     87.9M      0      0  00 [------]     16      0      0  00 [------]
myquota1      12K      0      0  00 [------]      7      0      0  00 [------]
myquota2      12K      0      0  00 [------]      7      0      0  00 [------]
myquota3      12K      0      0  00 [------]      7      0      0  00 [------]
myquota4      12K      0      0  00 [------]      7      0      0  00 [------]
myquota5      12K      0      0  00 [------]      7      0      0  00 [------]

# Blocks 是容量限制
# Inodes 是文件数量限制
# soft/hard 为 0 则表示没有限制

默认情况下， report 会将支持的 user/group/prject 相关数据都列出来；上面加了 -u 参数，所以只列出了 user 的相关数据
```

```bash
# 范例 4：列出目前支持的 quota 文件系统是否有启动了 quota 功能
[root@study ~]# xfs_quota -x -c "state" 
User quota state on /home (/dev/mapper/centos-home)
  Accounting: ON		# 是否启用计算功能； ON 是，OFF 否
  Enforcement: ON		# 是否有实际 quota 管制功能
  Inode: #1605 (2 blocks, 2 extents)
Group quota state on /home (/dev/mapper/centos-home)
  Accounting: ON
  Enforcement: ON
  Inode: #1606 (2 blocks, 2 extents)
Project quota state on /home (/dev/mapper/centos-home)	# project 并未开启
  Accounting: OFF	
  Enforcement: OFF
  Inode: #1606 (2 blocks, 2 extents)
Blocks grace time: [7 days]		# 下面是 grace timec 的项目，宽限时间
Inodes grace time: [7 days]
Realtime Blocks grace time: [7 days]

```

## 实验流程 3：限制值设置方式

启动支持并能查询到相关状态，那么进行实验的限制操作：每个用户 250/300MB 容量限制、群组供 950M/1G 容量限制、宽限时间（grace time）14 天

语法如下：

```bash
xfs_quota -x -c "limit [-ug] b[soft|hard]=N name"
xfs_quota -x -c "timer [-ug] [bir] Ndays"

选项与参数：
limit： 实际限制的项目，可以针对 user/group 限制，项目有：
	bsoft/bhard：block 的 soft/hard 限制值，可以加单位
	isoft/ihard：inode 的 soft/hard 限制值
	name：用户/群组名称
	
timer：设置 grace time 项目，可以针对 user/group 以及 block/inode 设置
```

```bash
# 范例 1：设置用户的 block 限制值（需求中没有说限制文件数量 inode）
[root@study ~]# xfs_quota -x -c "limit -u bsoft=250M bhard=300M myquota1" /home
[root@study ~]# xfs_quota -x -c "limit -u bsoft=250M bhard=300M myquota2" /home
[root@study ~]# xfs_quota -x -c "limit -u bsoft=250M bhard=300M myquota3" /home
[root@study ~]# xfs_quota -x -c "limit -u bsoft=250M bhard=300M myquota4" /home
[root@study ~]# xfs_quota -x -c "limit -u bsoft=250M bhard=300M myquota5" /home

[root@study ~]# xfs_quota -x -c "report -ubih" /home
User quota on /home (/dev/mapper/centos-home)
                        Blocks                            Inodes              
User ID      Used   Soft   Hard Warn/Grace     Used   Soft   Hard Warn/Grace  
---------- --------------------------------- --------------------------------- 
root            0      0      0  00 [------]      8      0      0  00 [------]
mrcode       7.4M      0      0  00 [------]    260      0      0  00 [------]
mrcode1     87.9M      0      0  00 [------]     16      0      0  00 [------]
myquota1      12K   250M   300M  00 [------]      7      0      0  00 [------]
myquota2      12K   250M   300M  00 [------]      7      0      0  00 [------]
myquota3      12K   250M   300M  00 [------]      7      0      0  00 [------]
myquota4      12K   250M   300M  00 [------]      7      0      0  00 [------]
myquota5      12K   250M   300M  00 [------]      7      0      0  00 [------]
# 可以看到  5 个账户的设置以及生效了，在 Blocks 这边
```

```bash
# 范例 2：设置 myqutarp 的 block 限制值
[root@study ~]# xfs_quota -x -c "limit -g bsoft=950M bhard=1G myquotagrp" /home

[root@study ~]# xfs_quota -x -c "report -gbih" /home
Group quota on /home (/dev/mapper/centos-home)
                        Blocks                            Inodes              
Group ID     Used   Soft   Hard Warn/Grace     Used   Soft   Hard Warn/Grace  
---------- --------------------------------- --------------------------------- 
root            0      0      0  00 [------]      7      0      0  00 [------]
mrcode       7.4M      0      0  00 [------]    260      0      0  00 [------]
mrcode1     87.9M      0      0  00 [------]     16      0      0  00 [------]
myquotagrp    60K   950M     1G  00 [------]     36      0      0  00 [------]

```

```bash
# 范例 3： 设置 grace time 为 14 天
[root@study ~]# xfs_quota -x -c "timer -u -b 14days" /home
[root@study ~]# xfs_quota -x -c "timer -g -b 14days" /home

[root@study ~]# xfs_quota -x -c "state" /home
User quota state on /home (/dev/mapper/centos-home)
  Accounting: ON
  Enforcement: ON
  Inode: #1605 (2 blocks, 2 extents)
Group quota state on /home (/dev/mapper/centos-home)
  Accounting: ON
  Enforcement: ON
  Inode: #1606 (2 blocks, 2 extents)
Project quota state on /home (/dev/mapper/centos-home)
  Accounting: OFF
  Enforcement: OFF
  Inode: #1606 (2 blocks, 2 extents)
Blocks grace time: [14 days]		# 这里变成了 14 天
Inodes grace time: [7 days]
Realtime Blocks grace time: [7 days]
```

```bash
# 范例 4：以 myquota1 的用户测试 quota 是否真的有限制效果
su -l myquotal
[myquota1@study ~]$ dd if=/dev/zero of=123.img bs=1M count=310
dd: 写入"123.img" 出错: 超出磁盘限额
记录了300+0 的读入
记录了299+0 的写出
313524224字节(314 MB)已复制，0.386459 秒，811 MB/秒
# 英文版本
[myquota1@study ~]$ dd if=/dev/zero of=123.img bs=1M count=310
dd: error writing '123.img': Disk quota exceeded
300+0 records in
299+0 records out
313524224 bytes (314 MB) copied, 0.175228 s, 1.8 GB/s

# 查看状态
[root@study ~]# xfs_quota -x -c "report -ubih" /home
User quota on /home (/dev/mapper/centos-home)
                        Blocks                            Inodes              
User ID      Used   Soft   Hard Warn/Grace     Used   Soft   Hard Warn/Grace  
---------- --------------------------------- --------------------------------- 
root            0      0      0  00 [0 days]      8      0      0  00 [------]
mrcode       7.4M      0      0  00 [------]    260      0      0  00 [------]
mrcode1     87.9M      0      0  00 [------]     16      0      0  00 [------]
myquota1   299.0M   250M   300M  00 [13 days]     16      0      0  00 [------]
myquota2      12K   250M   300M  00 [------]      7      0      0  00 [------]
myquota3      12K   250M   300M  00 [------]      7      0      0  00 [------]
myquota4      12K   250M   300M  00 [------]      7      0      0  00 [------]
myquota5      12K   250M   300M  00 [------]      7      0      0  00 [------]
# 会发现 myquota1 使用量已经满了，并且宽限时间还有 13 天
```

## 实验流程 4：project 的限制（针对目录）- 可选

注意：这里是针对  /home/myquota 这个目录，而不是针对 myquotagrp 这个群组。上面配置过 myquotagrp 这个组的限制

另外：上述 myquota1 已经使用了 300M 的容量，他是在他自己家的目录做的 dd 指令，因此 `/home/myquota` 这个目录目前还未使用，但是

```bash
[root@study ~]# xfs_quota -x -c "report -h" /home
User quota on /home (/dev/mapper/centos-home)
                        Blocks              
User ID      Used   Soft   Hard Warn/Grace   
---------- --------------------------------- 
root            0      0      0  00 [0 days]
mrcode       7.4M      0      0  00 [------]
mrcode1     87.9M      0      0  00 [------]
myquota1   299.0M   250M   300M  00 [13 days]
myquota2      12K   250M   300M  00 [------]
myquota3      12K   250M   300M  00 [------]
myquota4      12K   250M   300M  00 [------]
myquota5      12K   250M   300M  00 [------]

Group quota on /home (/dev/mapper/centos-home)
                        Blocks              
Group ID     Used   Soft   Hard Warn/Grace   
---------- --------------------------------- 
root            0      0      0  00 [0 days]
mrcode       7.4M      0      0  00 [------]
mrcode1     87.9M      0      0  00 [------]
myquotagrp 299.1M   950M     1G  00 [------]
```

会发现，myquotagrp 这个群组的容量已经用掉了 300M，对于目录的限制来说，就不会有效果了

还因为 project 不能与 group 同时设置，所以需要取消掉 group 的设置，并加入 project 才可以

```bash
# 修改 /etc/fstab 内的文件系统支持参数
# 把 grpquota 去掉，加入 prjquota
[root@study ~]# vim /etc/fstab 

/dev/mapper/centos-home /home                   xfs     defaults,usrquota,prjquota        0 0

# 卸载 /home, 并重新加载
# 这里还是老问题，远程登录笔者用 mrcode 为入口的，所以 /home 目录卸载在使用（虽然使用 su 切换到了 root）
# 这里还是使用重启机器来自动挂载
[root@study ~]# umount /home/
umount: /home: target is busy.
        (In some cases useful info about processes that use
         the device is found by lsof(8) or fuser(1))
[root@study ~]# mount -a

[root@study ~]# xfs_quota -x -c "state"
User quota state on /home (/dev/mapper/centos-home)
  Accounting: ON
  Enforcement: ON
  Inode: #1605 (2 blocks, 2 extents)
Group quota state on /home (/dev/mapper/centos-home)
  Accounting: OFF				# 已经关闭了
  Enforcement: OFF
  Inode: #1606 (2 blocks, 2 extents)
Project quota state on /home (/dev/mapper/centos-home)
  Accounting: ON		# 已经启动了
  Enforcement: ON
  Inode: #1606 (2 blocks, 2 extents)
Blocks grace time: [14 days]
Inodes grace time: [7 days]
Realtime Blocks grace time: [7 days]

# 这里还没有配置任何项目的限制的哈。除了第一行哪个 #0
[root@study ~]# xfs_quota -x -c "report -h" /home
User quota on /home (/dev/mapper/centos-home)
                        Blocks              
User ID      Used   Soft   Hard Warn/Grace   
---------- --------------------------------- 
root            0      0      0  00 [------]
mrcode       7.4M      0      0  00 [------]
mrcode1     87.9M      0      0  00 [------]
myquota1   299.0M   250M   300M  00 [13 days]
myquota2      12K   250M   300M  00 [------]
myquota3      12K   250M   300M  00 [------]
myquota4      12K   250M   300M  00 [------]
myquota5      12K   250M   300M  00 [------]

Project quota on /home (/dev/mapper/centos-home)
                        Blocks              
Project ID   Used   Soft   Hard Warn/Grace   
---------- --------------------------------- 
#0         394.3M      0      0  00 [------]
```

规范目录、项目名称和项目 ID

```bash
# 指定项目表伺服与目录的对应;
# 语法可以看成是：别名:目录；这里别名设置的是数字 11
[root@study ~]# echo "11:/home/myquota" >> /etc/projects
# 规范项目名称；
# 语法是：项目名称:目录别名
echo "myquotaproject:11" >> /etc/projid

# 这个语法感觉很奇怪，需要先个目录取个别名，然后再取个别名取关联这个别名
# 不过笔者看目录名称：可以这样来理解：
# projects 目录给目标目录取了一个  别名
# projid 用自定义的 ID，关联了别名
# 这个应该是解耦配置之类的考虑吧

# 初始化专案名称
[root@study ~]# xfs_quota -x -c "project -s myquotaproject"
Setting up project myquotaproject (path /home/myquota)...
Processed 1 (/etc/projects and cmdline) paths for project myquotaproject with recursion depth infinite (-1).
Setting up project myquotaproject (path /home/myquota)...
Processed 1 (/etc/projects and cmdline) paths for project myquotaproject with recursion depth infinite (-1).
Setting up project myquotaproject (path /home/myquota)...
Processed 1 (/etc/projects and cmdline) paths for project myquotaproject with recursion depth infinite (-1).

# 就可以看到完整的文件系统各项支持，与 project 的目录对应
[root@study ~]# xfs_quota -x -c "print " /home
Filesystem          Pathname
/home               /dev/mapper/centos-home (uquota, pquota)
/home/myquota       /dev/mapper/centos-home (project 11, myquotaproject)

[root@study ~]# xfs_quota -x -c "report -pbih " /home
Project quota on /home (/dev/mapper/centos-home)
                        Blocks                            Inodes              
Project ID   Used   Soft   Hard Warn/Grace     Used   Soft   Hard Warn/Grace  
---------- --------------------------------- --------------------------------- 
#0         394.3M      0      0  00 [------]    327      0      0  00 [------]
myquotaproject      0      0      0  00 [------]      1      0      0  00 [------]
# 第二行，是我们刚刚配置的项目 ID，各个使用都还是 0 
```

测试与观察

```bash
# 测试与观察
[root@study ~]# xfs_quota -x -c "limit -p bsoft=450M bhard=500M myquotaproject" /home
[root@study ~]# xfs_quota -x -c "report -pbih " /home
Project quota on /home (/dev/mapper/centos-home)
                        Blocks                            Inodes              
Project ID   Used   Soft   Hard Warn/Grace     Used   Soft   Hard Warn/Grace  
---------- --------------------------------- --------------------------------- 
#0         394.3M      0      0  00 [------]    327      0      0  00 [------]
myquotaproject      0   450M   500M  00 [------]      1      0      0  00 [------]
# 可以看到一件配置上 450 和 500 m 的限制了
dd if=/dev/zero of=/home/myquota/123/img bs=1M count=510

# 测试是否有效
[root@study ~]# dd if=/dev/zero of=/home/myquota/123.img bs=1M count=510
dd: error writing '/home/myquota/123.img': No space left on device
501+0 records in
500+0 records out
524288000 bytes (524 MB) copied, 0.491772 s, 1.1 GB/s
# 使用 root 对该目录写入，超过限制也被提示了，这个才是完整的针对目录的规范
```

小节：

- `/etc/projects`：给具体的目录建立别名
- `/etc/projid`：给目录别名建立 projid

这样就可以针对目录来限制了。

有如下一个常见，就能满足使用限制目录的需求：在 www 服务中，要针对某些目录进行容量限制，但是因为容量之前仅针对用户进行限制，但是由于 www 服务都是一个名为 httpd 的账户管理的，因此该 www 服务所产生的文件数据都属于 httpd 账户，就无法针对某些特定的目录进行限制了，有了 project 之后，就可以针对不同的目录做容量限制，而不用管在里面建立文件的文件拥有者

## XFS quota 的管理与额外指令对照表

这里讲解稍微完讲解常用的 quota 维护指令

- disable：暂时取消 quota 的限制，但其实同还是在计算 quota，只是暂时不对限制做任何操作

- enable：恢复 quota 的限制

- off：完全关闭 quota 的限制

  该选项只能通过卸载重新挂载才能重新开启 quota 的限制哟

- remov：必须要在 off 状态下才能执行该指令，移除 quota 的限制配置

  比如要移除 project 的限制配置，只需要 `remove -p` 就可以了，而不需要重新设置限制值为 0

实践练习

```bash
# 1. 暂时关闭 XFS 文件系统的 quota 限制功能
[root@study ~]# xfs_quota -x -c "disable -up" /home
[root@study ~]# xfs_quota -x -c "state " /home
User quota state on /home (/dev/mapper/centos-home)
  Accounting: ON
  Enforcement: OFF		# 已经变成 OFF 了
  Inode: #1605 (2 blocks, 2 extents)
Group quota state on /home (/dev/mapper/centos-home)
  Accounting: OFF
  Enforcement: OFF
  Inode: #1606 (2 blocks, 2 extents)
Project quota state on /home (/dev/mapper/centos-home)
  Accounting: ON
  Enforcement: OFF
  Inode: #1606 (2 blocks, 2 extents)
Blocks grace time: [14 days]
Inodes grace time: [7 days]
Realtime Blocks grace time: [7 days]

# 测试写入大于限制值的文件到 目录
[root@study ~]# dd if=/dev/zero of=/home/myquota/123.img bs=1M count=510
510+0 records in
510+0 records out
534773760 bytes (535 MB) copied, 0.365973 s, 1.5 GB/s
# 发现没有限制了

[root@study ~]# xfs_quota -x -c "report -pbih " /home
Project quota on /home (/dev/mapper/centos-home)
                        Blocks                            Inodes              
Project ID   Used   Soft   Hard Warn/Grace     Used   Soft   Hard Warn/Grace  
---------- --------------------------------- --------------------------------- 
#0         394.3M      0      0  00 [------]    327      0      0  00 [------]
myquotaproject   510M   450M   500M  00 [-none-]      2      0      0  00 [------]
# 会发现有统计，只是没有限制

# 开启限制
[root@study ~]# xfs_quota -x -c "enable -up" /home

# 再次写入，限制出现了
[root@study ~]# dd if=/dev/zero of=/home/myquota/123.img bs=1M count=510
dd: error writing '/home/myquota/123.img': No space left on device
501+0 records in
500+0 records out
524288000 bytes (524 MB) copied, 0.400271 s, 1.3 GB/s

# 关闭了 quota 限制行为，再尝试开启，会发现不能开启
[root@study ~]# xfs_quota -x -c "off -up" /home
[root@study ~]# xfs_quota -x -c "enable -up" /home
XFS_QUOTAON: Function not implemented
# 通过 off 之后，需要重新卸载挂载  umount/mount 才可以重新开启

# 笔者这里还是使用重启来替代者两个命令
umount /home; mount -a;

# 移除所有的限制
[root@study ~]#xfs_quota -x -c "off -up" /homee
# 移除项目的配置
[root@study ~]# xfs_quota -x -c "remove -p" /home

# 关闭后，不重新挂载，没有任何信息
[root@study ~]# xfs_quota -x -c "state" /home
[root@study ~]# xfs_quota -x -c "state"      
[root@study ~]# xfs_quota -x -c "report -pbh" /home
# 重新挂载后，查看，项目的限制配置就没有了
[root@study ~]# xfs_quota -x -c "report -pbh" /home
Project quota on /home (/dev/mapper/centos-home)
                        Blocks              
Project ID   Used   Soft   Hard Warn/Grace   
---------- --------------------------------- 
#0         394.3M      0      0  00 [------]
myquotaproject   500M      0      0  00 [------]

```

下面是 EXT 家族与 XFS 系统的相关命令，可以尝试使用以下命令来维护

设定流程项目 | XFS 文件系统 | EXT 家族
--|--|--
`/etc/fstab` 参数设置|usrquota/grpquota/prjquota|usrquota/grpquota
quota 配置文件|不需要|quotacheck
设置用户/群组限制值|`xfs_quota -x -c "limit "`|edquota 或 setquota
设置 grace time|`xfs_quota -x -c "timer "`|edquota
设置目录限制值|`xfs_quota -x -c "limit "`|无
观察报告|`xfs_quota -x -c "report "`|repquota 或 quota
启动与关闭 quota 限制|`xfs_quota -x -c "[disable|enable] "`|quotaoff、quotaon
发送警告给用户|目前版本尚未支持|warnquota

## 不修改现有系统的 quota

有这样一种情况：之前分区时，没有规划邮件信箱目录 `/var/spool/mail/` 是一个独立的 partition，现在主机也没有办法新增或分区出任何新的分区槽了。

那么在这样的情况下，quota 的支持与文件系统有关，无法夸文件系统来设计 quota 的 project 功能

那么此时，那么想要使用的邮件信箱与家目录的总体磁盘使用量为固定，要怎么做？由于 `/home/`和 `/var/spool/mail` 根本不可能是同一个 filesystem（除非是都不分区，使用根目录，才有肯可能整合在一起），所以这样的情况怎么进行 quota 的限制？

假设。你有 /home 这个独立的分区槽了，你只需要：

1. 将 `/var/spool/mail`这个目录完整的移动到 `/home` 下
2. 利用 `ln -s /home/mail /var/spool/mail` 来建立链接
3. 将 `/home` 进行 quota 的限制配置

还可以依据不同的使用者与群组来设置 quota，让后以同样的方式来进行 link

## 软件磁盘阵列（Software RAID）

## 什么是 RAID

Redundant Arrays of Inexpensive Disks 简称 RAID，翻译为：容错式链家磁盘阵列。

RAID 通过一种技术（软件或硬件），将多个较小的磁盘整合成为一个较大的磁盘，而且还具有数据保护功能。根据等级不同，整合后的磁盘具有不同的功能，基本常见的 level 有如下几种：

### RAID-0 等量模式 stripe：效能最佳

该模式如果使用相同型号与容量的磁盘来组成时，效果较佳。RAID 将磁盘切出等量的区块（chunk），一般可设置为 4K~1M 之间，当一个文件要写入 RAID 时，该文件会依据 chunk 的大小切割好，再依序放到各个磁盘里。会优先等量存放到各个磁盘上去，比如：有两块磁盘都是 100G，你有一个文件 100M，那么每个磁盘会分到 50MB 的容量，但是如果一块磁盘是 50G，另外一块是 100G，由于是等量，但是由于磁盘小磁盘先被放满，那么其他数据就会落到另一块磁盘上，这种情况下效能就降低了，因为所有压力由一块磁盘承担了。

![image-20200227143813650](http://p4ui.toweydoc.tech:20080/images/stydocs/image-20200227143813650.png)

这种模式下，越多的硬盘组成的 RAID-0 理论上效能会越好，因为每块磁盘负责的数据量更低了，但是 **只要有任何一块磁盘顺坏，在 RAID 上面的所有数据都会丢失而无法读取**。

因为等量分散在每块硬盘上，坏掉一块硬盘，意味着有部分数据丢失了，那么一个文件的部分数据丢失了，不完整了，可能该文件也就损坏了

### RAID-1 映像模式 mirror：完整备份

建议使用相同磁盘容量，最好是一样的磁盘，如果是不同容量的磁盘组成，那么总容量将以最小的那一块为主。

这种模式主要是：让同一份数据，完整的保存在两块磁盘上。比如：100MB 的文件，且目前只有 两块硬盘组成 RAID-1 时，那么每块硬盘上都会写入 100MB 的数据。由于两块磁盘内容一模一样，好像镜子映照出来一样，所以也称为 mirror 模式

![image-20200227144535061](http://p4ui.toweydoc.tech:20080/images/stydocs/image-20200227144535061.png)

如图，一份数据被写入到两块磁盘中，如果使用软件来做磁盘阵列，那么效能会大大降低，因为只有一个南桥，一份数据要写入两次。如果使用的是 RAID（磁盘阵列卡）时，阵列卡会主动复制一份儿不使用系统的 I/O 总线，效能就还可以。

由于是映像数据，当其中一块硬盘损坏，数据还是完整的。

RAID-1 最大的优点大概就在于数据的备份。用空间换安全。写入效能不佳，但是读取效能还不错，多个 processes 在读取同一份数据时，RAID 会自行取得最佳的读取平衡

### RAID 1+0，RAID 0+1

RAID-0 效能佳，数据不安全。RAID-1 数据安全，但效能不佳。将两者组合起来就形成了：

- RAID 1 + 0：先让两块磁盘组成 RAID 1（共有两组），再让这两组 RAID 1 再组成一组 RAID 0
- RAID 0 + 1：则与上面相反

![image-20200227145429427](http://p4ui.toweydoc.tech:20080/images/stydocs/image-20200227145429427.png)

此种方式，最坏的可能性是同时坏掉 b 和 c，也就是 RAID 1 中的其中一块。那么数据都能保证完整。此种方式也是目前存储设备厂商推荐的方法

假设你有 20 快磁盘组成的系统，每两块组成一个 RAID 1，总共有 10 组可以自己复原的系统，再让这 10 组组成一个新的 RAID 0，速度立刻拉升 10 倍了。同时注意：因为每组 RAID 1 是个别独立存在的，因此任何一块磁盘损坏，数据都是从另一块磁盘直接复制过来重建，并不像 RAID5/RAID6 必须要整组 RAID 的磁盘共同重建一块独立的磁盘系统，而且 RAID 1 与 RAID 0 不需要经过计算的（striping），读写效率也比其他的 RAID 等级好太多了，这也是为什么会推荐 RAID1 + 0 方式了

### RAID 5：效能与数据备份的均衡考虑

RAID 5 至少需要三块以上的磁盘才能够组成这种类型的磁盘阵列。数据写入类似 RAID 0，不过每个循环的写入过程中（striping），在每块磁盘还加入一个同位检查数据（Parity），该数据会记录其他磁盘的备份数据，用于当有磁盘损坏时的救援。

![image-20200227150303026](http://p4ui.toweydoc.tech:20080/images/stydocs/image-20200227150303026.png)

每个循环写入时，都会有部分的同为检查码 parity 被记录，并且记录的同位检查码每次都记录在不同的磁盘，当任何一个磁盘损坏时，可以由其他磁盘的检查码来重建原磁盘内的数据，由于有同位检查码，RAID 5 的总容量会是整体磁盘数量减 1 快磁盘。原本 3 块磁盘，只剩下 3-1 =2 快磁盘的容量。当磁盘损坏数量大于等于 2 块时，整组 RAID 5 的数据就损坏了。因此 RAID 5 预设只能支持一块磁盘损坏的情况

读写效能上的比较：

- 读：由于多快磁盘，效率堪比 RAID-0
- 写：有同位检查码 parity的计算，写入效能与系统的硬件关系较大，尤其当使用软件磁盘阵列时，parity 是通过 CPU 去计算而非专职的磁盘阵列卡，所以写效率需要评估

RAID 5 仅能支持一块磁盘的损坏，因此出现了 RAID 6，RAID 6 使用 2 块磁盘作为 parity 的存储。所以当两块磁盘损坏时，数据还完整

### Spare Disk：预备磁盘的功能

当磁盘阵列损坏时，需要更换新的硬盘，并重建原本的数据。

更换上新的硬盘并顺利启动磁盘阵列后，磁盘阵列会主动重建（rebuild）原本损坏那块磁盘的数据到新的磁盘上，这是磁盘阵列的有点。必过，需要手动插拨硬盘（更换坏掉的硬盘），如果你的系统支持热插播，那么不需要关机就可以完成数据重建

为了让系统可以实时的在坏掉硬盘时主动重建，就需要预备磁盘（spare disk）。就是一块或多块磁盘不包含在原本的磁盘阵列等级中的磁盘，当磁盘阵列中的磁盘顺坏时，则会被主动拉进磁盘阵列中，将坏的磁盘移除磁盘阵列，并重建数据。

同样，如果你的磁盘阵列支持热插拨，拔掉旧硬盘，换上新硬盘，并设置为 spare disk。就更方便了

举例来说“有一个磁盘阵列可允许 16 快磁盘的数量，只有 10 块硬盘（每块 250G），用了 9 快硬盘组成 RAID 5。 1 快硬盘作为 spare disk，那么该磁盘阵列总容量为  (9-1)*250G=2000G。两年后看信号灯才发现坏掉了一块磁盘，那么之前预备的那块 spare disk 被自动拉进磁盘阵列了。 对数据没有任何影响

### 磁盘阵列的优点

如果要磁盘阵列，应该考虑：

1. 数据安全与可靠性：不是指网络信息安全，而是当硬件（磁盘）损坏时，数据是否能保证不丢失
2. 读写效率：例如 RAID 0 可以加强读写效率，让你的系统 I/O 部分得到改善
3. 容量：可以让多块磁盘组合起来，也就意味着单一文件系统可以有相当大的容量

尤其是数据的可靠性与完整性更是使用 RAID 的考虑重点。

各个等级优缺点如下

项目 | RAID 0 | RAID 1 | RAID 10 | RAID 5 | RAID 6
:-:|:-:|:-:|:-:|:-:|:-:
最少磁盘|2|2|4|3|4
最大容错磁盘（1）|无|n-1|n/2|1|2
数据安全性（1）|完全没有|最佳|最佳|好|比 RAID5 好
理论写入效率（2）|n|1|n/2|< n-1|< n-2
理论读取效率（2）|n|n|n|< n-1|< n-2
可用容量（3）|n|1|n/2|n-1|n -2
一般应用|强调效率单数据不重要|资料与备份|服务器、云系统常用|资料与备份|资料与备份

注意：因为 RAID5/6 读写需要经过 parity 的计算器，因此读写效率不会刚好满足使用的磁盘数量

另外，根据使用情况的不同，一般推荐的磁盘阵列的等级也不同；比如几百 GB 单一大文件数据，会选择在 RAID 6 ，能满足读写需求还有数据完整性保证。如果是在云程序环境中，因为需要确保每个虚拟机能够快速反应及提供数据完整性，因此 RAID5/6 效率较弱的等级是不考虑的，总结来说，大概就剩下  RAID 10 能满足云环境的效率需求了。在某些更特别的环境下，如果搭配 SSD 那么读写效率上会更好



## software、hardware RAID

磁盘阵列分为软件与硬件磁盘阵列

硬件磁盘阵列 hardware RAID 是通过磁盘阵列卡来达成数组的目的。上面有一块专门的芯片处理 RAID 任务，那么很多任务（如 RAID 5 的同为检查码计算）就不会重复消耗原本系统的 I/O 总线。理论上效率更好。另外一般的中高阶磁盘阵列卡都支持热插拨

磁盘阵列卡好的上万，低价的可能只能支持到 RAID 0 与 1，同时还需要磁盘阵列卡的驱动程序，才能使用

由于磁盘阵列有很多优秀的功能，但是很贵，因此就出现了软件仿真磁盘阵列功能，

软件磁盘阵列 software RAID 主要通过软件来仿真数组的任务，因此会耗费较多的系统资源，如 CPU 运算和 IO 总线等资源。由于个人计算机的发展快速，这些限制还不算严重，可以玩一玩软件磁盘阵列

以 CentOS 提供的软件 mdadm 磁盘阵列来说，它会以 partition 或 disk 为磁盘单位，意味着你不需要两块以上的磁盘，只需要有两个以上的分区槽（partition）就能过设计你的磁盘阵列。它支持的等级有 RAID0/1/5 spare disk 等，而且提供的管理机制还能达到类似热插拨的功能，可以在线（文件系统正常使用）进行分区槽的抽换。

硬件磁盘阵列在 Linux 下看起来就是一块实际的大磁盘，因此硬件磁盘阵列文件名为 `/dev/sd[a-p],因为使用到 SCSI 的模块的原因。

软件磁盘阵列时系统仿真的，因此使用的文件名是系统的装置文件，文件名为 `/dev/md0  /dev/md1 ...`

Intel 的南桥附赠的磁盘阵列功能，在 windows 视乎是完整的磁盘阵列，但是在 Linux 下被视为是软件磁盘阵列的一种，因此如果你有设置过 Intel 的南桥芯片磁盘阵列，那么在 Linux 下会是 `/dev/md126、/dev/md127` 之类的装置文件名，而他的分区槽是 `/dev/md126p1、/dev/md126p2` 之类

## 磁盘阵列的设置

很简单，一个指令即可

```bash
mdadm --detail /dev/md0
mdadm --create /dev/md[0-9] --auto=yes --level=[015] --chunk=NK --raid-devices=N --spare-devices=N /dev/sdx /dev/hdx...

选项与参数：

	--create：创建 RAID 关键词
	--auto=yes：决定建立后面接的软件磁盘阵列装置。也就是 `/dev/md0 、/dev/md1 ...`
	--chunk=NK：决定装置的 chunk 大小，也可以当成 stripe 大小，一般是 64K 或 512K
	--raid-devices=N：使用几个磁盘（partition）作为磁盘阵列的装置
	--spare-devices=N：使用几个磁盘作为备用（spare）装置
	--level=[015]：设置这组磁盘阵列的等级。支持很多，不过建议只要用 0、1、5
	--detail：后面接的那个磁盘阵列装置的详细信息
```

如上语法中，谈到后面接装置名，这些装置文件可以是整块磁盘（如 /dev/sdb），也可以是分区槽（如 `/dev/sdb1`） 之类，但是这些装置文件名的总数必须要等于 `--raid-devices`  与 `--spare-devices` 的个数综合才行。

下面使用测试机创建一个 RAID5 的软件磁盘阵列，磁盘阵列规划如下：

- 利用 4 个 partition 组成 RAID5
- 每个 partition 约 1GB，需确定每个 partition 一样大较佳
- 利用 1 个 partition 设置为 spare disk
- chunk 设置为 256K
- spare disk 的大小与其他 RAID 所需 partition 一样大
- 将此 RAID 5 装置挂载到 `/srv/raid` 目录下

### 磁盘分区

下面进行分区，[分区手段参考之前的的章节](../07/03)

```bash
# 查看当前磁盘状态
[root@study ~]# lsblk
NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda               8:0    0 40.8G  0 disk 
├─sda1            8:1    0    2M  0 part 
├─sda2            8:2    0    1G  0 part /boot
└─sda3            8:3    0   30G  0 part 
  ├─centos-root 253:0    0   10G  0 lvm  /
  ├─centos-swap 253:1    0    1G  0 lvm  [SWAP]
  └─centos-home 253:2    0    5G  0 lvm  /home
sdb               8:16   0    2G  0 disk 
sr0              11:0    1 73.6M  0 rom  
# 上面发现一快 sda 磁盘，有三个分区，分区大小是 33 G 左右，也就是说还有 7 G 左右可以使用
[root@study ~]# lsblk -ip /dev/sda
NAME                        MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
/dev/sda                      8:0    0 40.8G  0 disk 
|-/dev/sda1                   8:1    0    2M  0 part 
|-/dev/sda2                   8:2    0    1G  0 part /boot
`-/dev/sda3                   8:3    0   30G  0 part 
  |-/dev/mapper/centos-root 253:0    0   10G  0 lvm  /
  |-/dev/mapper/centos-swap 253:1    0    1G  0 lvm  [SWAP]
  `-/dev/mapper/centos-home 253:2    0    5G  0 lvm  /home

# 那么下面
[root@study ~]# lsblk -ip /dev/sda
NAME                        MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
/dev/sda                      8:0    0 40.8G  0 disk 
|-/dev/sda1                   8:1    0    2M  0 part 
|-/dev/sda2                   8:2    0    1G  0 part /boot
`-/dev/sda3                   8:3    0   30G  0 part 
  |-/dev/mapper/centos-root 253:0    0   10G  0 lvm  /
  |-/dev/mapper/centos-swap 253:1    0    1G  0 lvm  [SWAP]
  `-/dev/mapper/centos-home 253:2    0    5G  0 lvm  /home
  
# 开始分区，总共需要分出来 5 个 1 g 的分区
[root@study ~]# gdisk /dev/sda
GPT fdisk (gdisk) version 0.8.10

Partition table scan:
  MBR: protective
  BSD: not present
  APM: not present
  GPT: present

Found valid GPT with protective MBR; using GPT.
# 查看当前 磁盘 的状态
Command (? for help): p
Disk /dev/sda: 85491712 sectors, 40.8 GiB
Logical sector size: 512 bytes
Disk identifier (GUID): 67038DBF-B66A-4D0F-92B2-BFBF0744CD1D
Partition table holds up to 128 entries
First usable sector is 34, last usable sector is 85491678
Partitions will be aligned on 2048-sector boundaries
Total free space is 20467645 sectors (9.8 GiB)		# 空闲扇区

Number  Start (sector)    End (sector)  Size       Code  Name
   1            2048            6143   2.0 MiB     EF02  
   2            6144         2103295   1024.0 MiB  0700  
   3         2103296        65026047   30.0 GiB    8E00  

# 有上面可以看出来，实际上还有 9 g 左右可以使用
Command (? for help): n
Partition number (4-128, default 4): 
First sector (34-85491678, default = 65026048) or {+-}size{KMGTP}: 
Last sector (65026048-85491678, default = 85491678) or {+-}size{KMGTP}: +1G
Current type is 'Linux filesystem'
Hex code or GUID (L to show codes, Enter = 8300): Fd00
Changed type of partition to 'Linux RAID'
# 上面分区了一个，后面 4 个分区，就不贴出来了
....
Command (? for help): P
Disk /dev/sda: 85491712 sectors, 40.8 GiB
Logical sector size: 512 bytes
Disk identifier (GUID): 67038DBF-B66A-4D0F-92B2-BFBF0744CD1D
Partition table holds up to 128 entries
First usable sector is 34, last usable sector is 85491678
Partitions will be aligned on 2048-sector boundaries
Total free space is 9981885 sectors (4.8 GiB)

Number  Start (sector)    End (sector)  Size       Code  Name
   1            2048            6143   2.0 MiB     EF02  
   2            6144         2103295   1024.0 MiB  0700  
   3         2103296        65026047   30.0 GiB    8E00  
   4        65026048        67123199   1024.0 MiB  FD00  Linux RAID
   5        67123200        69220351   1024.0 MiB  FD00  Linux RAID
   6        69220352        71317503   1024.0 MiB  FD00  Linux RAID
   7        71317504        73414655   1024.0 MiB  FD00  Linux RAID
   8        73414656        75511807   1024.0 MiB  FD00  Linux RAID
# 最后分区好的 5 个分区如上。每个都占用 1G 空间
# 最后保存分区
Command (? for help): w

Final checks complete. About to write GPT data. THIS WILL OVERWRITE EXISTING
PARTITIONS!!

Do you want to proceed? (Y/N): y
OK; writing new GUID partition table (GPT) to /dev/sda.
Warning: The kernel is still using the old partition table.
The new table will be used at the next reboot.
The operation has completed successfully.


# 再次确认分区
Command (? for help): p
Disk /dev/sda: 85491712 sectors, 40.8 GiB
Logical sector size: 512 bytes
Disk identifier (GUID): 67038DBF-B66A-4D0F-92B2-BFBF0744CD1D
Partition table holds up to 128 entries
First usable sector is 34, last usable sector is 85491678
Partitions will be aligned on 2048-sector boundaries
Total free space is 9981885 sectors (4.8 GiB)		# 空闲扇区

Number  Start (sector)    End (sector)  Size       Code  Name
   1            2048            6143   2.0 MiB     EF02  
   2            6144         2103295   1024.0 MiB  0700  
   3         2103296        65026047   30.0 GiB    8E00  
   4        65026048        67123199   1024.0 MiB  FD00  Linux RAID
   5        67123200        69220351   1024.0 MiB  FD00  Linux RAID
   6        69220352        71317503   1024.0 MiB  FD00  Linux RAID
   7        71317504        73414655   1024.0 MiB  FD00  Linux RAID
   8        73414656        75511807   1024.0 MiB  FD00  Linux RAID

# 之前空闲扇区是 9.8G，现在还有 4.8G,也有 5 个分区，证明是分区成功了的
# 最后记得分区相关的更新指令，否则使用 lsblk 还是看不到分区出来的 part 
```

上面复习了一下分区操作，下面开始配置 RAID

### 配置 RAID

```bash
# 确认分区槽
[root@study ~]# lsblk
NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda               8:0    0 40.8G  0 disk 
├─sda1            8:1    0    2M  0 part 
├─sda2            8:2    0    1G  0 part /boot
├─sda3            8:3    0   30G  0 part 
│ ├─centos-root 253:0    0   10G  0 lvm  /
│ ├─centos-swap 253:1    0    1G  0 lvm  [SWAP]
│ └─centos-home 253:2    0    5G  0 lvm  /home
├─sda4            8:4    0    1G  0 part 
├─sda5            8:5    0    1G  0 part 
├─sda6            8:6    0    1G  0 part 
├─sda7            8:7    0    1G  0 part 
└─sda8            8:8    0    1G  0 part 
sdb               8:16   0    2G  0 disk 
sr0              11:0    1 73.6M  0 rom 

# 以 mdadm 建立 RAID
[root@study ~]# mdadm --create /dev/md0 --auto=yes --level=5 --chunk=256K --raid-devices=4 --spare-devices=1 /dev/sda{4,5,6,7,8}
mdadm: /dev/sda5 appears to contain an ext2fs file system
       size=1048576K  mtime=Sun Oct 27 18:05:52 2019	# 有时候出现这个信息,不用管
Continue creating array? y
mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md0 started.
# 详细的参数说明，查看前面章节，这里使用 {} 将重复的项目简化
# 此外 dsa 5 提示 ext2fs 问题，是抓到之前的 filesystem
# 去人创建，就会删除之前的的旧系统

[root@study ~]# mdadm --detail /dev/md0
/dev/md0:																								# 装置文件名
           Version : 1.2
     Creation Time : Sat Feb 29 06:13:57 2020						# 创建 RAID 的时间
        Raid Level : raid5															# RAID 等级，这里是 RAID5
        Array Size : 3139584 (2.99 GiB 3.21 GB)					# 整组 RAID 整组可用容量
     Used Dev Size : 1046528 (1022.00 MiB 1071.64 MB)		# 每块磁盘的容量
      Raid Devices : 4																	# 组成 RAID 的磁盘数量
     Total Devices : 5																	# 包含 spare 的总磁盘数量
       Persistence : Superblock is persistent

       Update Time : Sat Feb 29 06:14:06 2020
             State : clean 															# 目前这个磁盘整理的使用状态
    Active Devices : 4																	# active 启动的装置数量
   Working Devices : 5																	# 目前使用此数组的数量
    Failed Devices : 0																	# 损坏的装置数量
     Spare Devices : 1																	# 预备磁盘的数量

            Layout : left-symmetric
        Chunk Size : 256K																# chunk 的区块容量

Consistency Policy : resync

              Name : study.centos.mrcode:0  (local to host study.centos.mrcode)
              UUID : ba4883f9:75e8224b:6961ac93:d16adbf7
            Events : 18

    Number   Major   Minor   RaidDevice State
       0       8        4        0      active sync   /dev/sda4
       1       8        5        1      active sync   /dev/sda5
       2       8        6        2      active sync   /dev/sda6
       5       8        7        3      active sync   /dev/sda7

       4       8        8        -      spare   /dev/sda8
# 最后的 5 行数据，是目前 5 快磁盘分区的情况，4 个 active sync，1 个 spare
# RaidDevice 指的是此 RAID 内的磁盘顺序
```

由于磁盘阵列创建需要一些时间，所以最好等待几分钟再使用该指令 `mdadm --detail /dev/md0`查看磁盘阵列情况，否则有可能看到某些磁盘正在 `spare rebuilding` 之类的提示

通过以上操作，就创建了一个 RAID5 且含有一块 spare disk 的磁盘阵列。

还可以通过如下的文件来查看系统软件磁盘阵列的情况

```bash
[root@study ~]# cat /proc/mdstat 
Personalities : [raid6] [raid5] [raid4] 
md0 : active raid5 sda7[5] sda8[4](S) sda6[2] sda5[1] sda4[0]
      3139584 blocks super 1.2 level 5, 256k chunk, algorithm 2 [4/4] [UUUU]
      
unused devices: <none>

```

上述属性信息， md0 的两行信息：

- 第一行：指出 md0 为 raid5，且使用了 sda7、sda6、sda5、sda4 四块磁盘。每个装置后面的中括号中的数字是此磁盘在 RAID 中的顺序（RaidDevice），`sda8[4](s)` 中的 s 则代表它为 spare 磁盘

- 第二行：此磁盘阵列有 3139584 blocks（每个 block 为 1k），所以总容量为 3GB，使用 readi 5 等级，写入磁盘的小区块 chunk 为 256K，使用 algorithm2 磁盘阵列算法。

  `[m/n]` 表示此数组需要 m 个装置，且 n 个装置正常运行。因此这里 需要 4 个装置且 4 个装置正常运作。

  `[UUUU]` 表示 4 个所需的装置的启动情况，U 表示运作，若为`_`则表示不正常

### 格式化与挂载使用 RAID

因为涉及到 xfs 文件系统的优化（第 7 章中有讲解到），所以这里参数为：

- srtipe（chunk）容量为 256k， su=256k
- 共有 4 块组成 RAID5，因此容量少一块， sw=3
- 由上面两项计算出数据宽度为  256K*3=768K

所以整体要优化这个 XFS 文件系统就是这样：

```bash
[root@study ~]# mkfs.xfs -f -d su=256k,sw=3 -r extsize=768k /dev/md0
meta-data=/dev/md0               isize=512    agcount=8, agsize=98048 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=0, sparse=0
data     =                       bsize=4096   blocks=784384, imaxpct=25
         =                       sunit=64     swidth=192 blks
naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
log      =internal log           bsize=4096   blocks=2560, version=2
         =                       sectsz=512   sunit=64 blks, lazy-count=1
realtime =none                   extsz=786432 blocks=0, rtextents=0

# 给该装置挂载到 /srv/raid 目录
[root@study ~]# mkdir /srv/raid
[root@study ~]# mount /dev/md0 /srv/raid/
[root@study ~]# df -Th /srv/raid/
文件系统       类型  容量  已用  可用 已用% 挂载点
/dev/md0       xfs   3.0G   33M  3.0G    2% /srv/raid

# 就看到多了一个 /dev/md0 的挂载装置了
```

## 仿真 RAID 错误的救援模式

万一磁盘阵列内的装置异常了，要怎么进行救援，下面进行仿真实践

### 语法

```bash
# mdadm 救援方面的语法
mdadm --manage /dev/md[0-9] [--add 装置] [--remove 装置] [--fail 装置]

选项与参数：
	--add：将装置加入到该 md 中
	--remove：将装置从该 md 中移除
	--fail：将某个装置设置为出错状态
```

### 设置磁盘为错误

让一块磁盘变成错误，然后让 spare disk 自动开始重建

```bash
# 先复制一点数据到磁盘阵列中
[root@study ~]# cp -a /etc /var/log/ /srv/raid/

[root@study ~]# df -Th /srv/raid/
Filesystem     Type  Size  Used Avail Use% Mounted on
/dev/md0       xfs   3.0G  121M  2.9G   4% /srv/raid
[root@study ~]# du -sm /srv/raid/*
42      /srv/raid/etc
45      /srv/raid/log
# 已经复制过来文件了，并且依据看到使用量了

# 1. 假设 /dev/sda6 这个磁盘出问题了，模拟方式如下
[root@study ~]# mdadm --manage /dev/md0  --fail /dev/sda6
mdadm: set /dev/sda6 faulty in /dev/md0
# 查看状态
[root@study ~]# mdadm --detail /dev/md0
/dev/md0:
           Version : 1.2
     Creation Time : Sat Feb 29 06:13:57 2020
        Raid Level : raid5
        Array Size : 3139584 (2.99 GiB 3.21 GB)
     Used Dev Size : 1046528 (1022.00 MiB 1071.64 MB)
      Raid Devices : 4
     Total Devices : 5
       Persistence : Superblock is persistent

       Update Time : Sat Feb 29 07:57:44 2020
             State : clean 
    Active Devices : 4
   Working Devices : 4
    Failed Devices : 1			#出错的有一个
     Spare Devices : 0

            Layout : left-symmetric
        Chunk Size : 256K

Consistency Policy : resync

              Name : study.centos.mrcode:0  (local to host study.centos.mrcode)
              UUID : ba4883f9:75e8224b:6961ac93:d16adbf7
            Events : 37

    Number   Major   Minor   RaidDevice State
       0       8        4        0      active sync   /dev/sda4
       1       8        5        1      active sync   /dev/sda5
       4       8        8        2      active sync   /dev/sda8
       5       8        7        3      active sync   /dev/sda7

       2       8        6        -      faulty   /dev/sda6
# 这里 sda6 死掉了，我这里查看信息慢了点，没有看到 state 为 spare rebuilding 状态的信息
# 前面通过  cat /proc/mdstat  看到的是 sda8 是 spare 磁盘，这里已经变成 磁盘整理里面的磁盘了

Personalities : [raid6] [raid5] [raid4] 
md0 : active raid5 sda7[5] sda8[4] sda6[2](F) sda5[1] sda4[0]
      3139584 blocks super 1.2 level 5, 256k chunk, algorithm 2 [4/4] [UUUU]
      
unused devices: <none>
# 可以看到 sda6 死掉了。变成了 F
```

### 将出错的磁盘移除并加入新磁盘

这里由于是让 sda6 模拟出错，如果是真实的盘要替换，也是如下的流程：

1. 从 `/dev/md0` 数组中移除  `/dev/sda6` 磁盘

2. 整个 Linux 系统关机，拔出 sda6 这块磁盘，并安装上新的 `/dev/sda6` 磁盘，之后开机

3. 将新的 `/dev/sda6` 放入 `/dev/md0` 数组中

4. 拔出旧的  sda6 磁盘

  ```bash
  # 这里我手误写成了 sda7 ,会提示在使用中的不能移除
  [root@study ~]# mdadm --manage /dev/md0 --remove /dev/sda7
  mdadm: hot remove failed for /dev/sda7: Device or resource busy
  # 这里就正常了
  [root@study ~]# mdadm --manage /dev/md0 --remove /dev/sda6
  mdadm: hot removed /dev/sda6 from /dev/md0

  [root@study ~]# mdadm --detail /dev/md0
      Number   Major   Minor   RaidDevice State
         0       8        4        0      active sync   /dev/sda4
         1       8        5        1      active sync   /dev/sda5
         4       8        8        2      active sync   /dev/sda8
         5       8        7        3      active sync   /dev/sda7
  # sda6 被移除了
  ```

5. 安装新的 `/dev/sda6` 磁盘

  ```bash
  [root@study ~]# mdadm --manage /dev/md0 --add /dev/sda6
  mdadm: added /dev/sda6
  [root@study ~]# mdadm --detail /dev/md0
  
     Number   Major   Minor   RaidDevice State
         0       8        4        0      active sync   /dev/sda4
         1       8        5        1      active sync   /dev/sda5
         4       8        8        2      active sync   /dev/sda8
         5       8        7        3      active sync   /dev/sda7
  
         6       8        6        -      spare   /dev/sda6
  
  ```



如果你的系统支持磁盘热插拨，还不需要关机，就能完成这里的操作，可以说是不停机完成维护，很强

## 开机自动启动 RAID 并自动挂载

新版 distribution 可能会自动搜索 `/dev/md[0-9]` 开机时设置好所需要的功能建议还是通过配置文件配置。

`/etc/mdadm.conf` 是 RAID 的配置文件。只要知道 `/dev/md0` 的 UUID 就可以完成配置

```bash
[root@study ~]# mdadm --detail /dev/md0 | grep -i uuid
              UUID : ba4883f9:75e8224b:6961ac93:d16adbf7
# UUID 是想系统注册的 UUID 标识

# 配置 mdadm.conf
[root@study ~]# vim /etc/mdadm.conf
ARRAY /dev/md0 UUID=ba4883f9:75e8224b:6961ac93:d16adbf7
# 		RAID 装置	UUID

# 设置开机自动挂载并测试
[root@study ~]# blkid /dev/md0
/dev/md0: UUID="319f3d68-a144-4d9a-9a83-c83b4a26b98f" TYPE="xfs"
[root@study ~]# vim /etc/fstab
# UUID=319f3d68-a144-4d9a-9a83-c83b4a26b98f       /srv/raid       xfs     0 0 
UUID=319f3d68-a144-4d9a-9a83-c83b4a26b98f /srv/raid     xfs     defaults        0 0
# 记得上述一定要仔细，这里卸载后，再挂载就提示有问题，检查后发现，少了一个 defaults 项
# 否则直接重启的话，有可能就启动不了了
[root@study ~]# umount /dev/md0; mount -a
mount: wrong fs type, bad option, bad superblock on /dev/md0,
       missing codepage or helper program, or other error

       In some cases useful info is found in syslog - try
       dmesg | tail or so.

# 修改后就正常了
[root@study ~]# umount /dev/md0
[root@study ~]# mount -a
[root@study ~]# df -Th /srv/raid/
Filesystem     Type  Size  Used Avail Use% Mounted on
/dev/md0       xfs   3.0G  121M  2.9G   4% /srv/raid
[root@study ~]# reboot 	# 重启后，再查看是否自动挂载了

```

## 关闭软件 RAID（重要）

除非你未来要使用这块  software RAID 的 /dev/md0；否则请关闭它

因为：因为这是练习机，后续不会用，另外，它使用了 `/dev/sda{4,5,6,7,8}` 分区，如果只是将 `/dev/md0` 卸载，然后忘记关闭 RAID，结果就是在重新分区 `/dev/sdaX` 时可能会出现一些莫名其妙的错误

关闭步骤如下

```bash
# 1. 卸载且删除配置文件与这个 /dev/md0 有关的设置
[root@study ~]# umount /srv/raid/
[root@study ~]# vim /etc/fstab 

# 2. 覆盖掉 RAID 的 metadata 以及 XFS 的 superblock，再关闭 /dev/md0
[root@study ~]# dd if=/dev/zero of=/dev/md0 bs=1M count=50
# 这一步很危险；笔者刚刚发现执行 stop,提示找不到 /dev/md0
# 找到问题是在 vim /etc/mdadm.conf 中 ARRAY 写错了，导致开机没有加载
# 然后修改之后，把 fstab 中增加上，再重启，就开不了机了，本想是重试这里的正常卸载
# 谁知道，上面吧  /dev/md0 数据擦除了，导致启动不了系统
# 后来想到前面讲解过怎么进入救援模式，再编辑 fstab 文件，去掉对 /dev/md0 的挂载配置，就好了

# 重启后，在执行下面的指令就正常了
[root@study ~]# mdadm --stop /dev/md0
mdadm:stopped /dev/md0
[root@study ~]# cat /proc/mdstat 
Personalities : [raid6] [raid5] [raid4] 
unused devices: <none>	# 不存在任何数组装置

# 在去掉这里的配置就可以了
[root@study ~]# vim /etc/mdadm.conf 

```

上面使用 dd 是因为 RAID 的相关数据会存一份在磁盘中，如果只是把配置文件去掉了，但是分区槽没有重新规划过，那么重启后，系统还是会将这块磁盘阵列建立起来，只是名称可能会改变成 `/dev/md127`

所以，记得上述的步骤，但是不要错误的 dd 到了其他磁盘，这个很严重的

##  逻辑滚动条管理员（Logical Volume Manager）

有一个场景：当初规划主机的时候，只给了 `/home` 目录 50G，后续不够用了，要怎么办？

很多人会这样做：加一块新硬盘，重新分区、格式化，将 `/home` 的数据完整复制过去，将原本的 partition 卸载重新挂载新的 partition。

万一第二次又分多了，导致容量浪费了，又要减掉一部分容量？是不是还需要重来一次？

有更简单的方法来解决这个问题，就是使用 LVM，它的重点在于「可以弹性的调整 filesystem 的容量，并非在于效能与数据保全上面。

LVM 可以整合多个实体 partition 在一起，让这些 partitions 看起来像是一个磁盘一样，而且可以在未来新增或移除其他的实体 partition 到这个 LVM 管理的磁盘中。

## 上面是 LVM：PV、PE、VG、LV 的意义

Logical Volume Manager 翻译为逻辑滚动条管理员，简称 LVM。LVM 将几个实体的 partitions（或 disk）通过软件组合成一块看起来是独立的大磁盘（**VG**）,然后将这块大磁盘再经过分区成为可使用分区槽（**LV**），最终就能挂载使用了。

但是为什么这样的系统可以进行 filesystem 的扩充或缩小呢？它与一个称为 **PE** 的项目有关

### Physical Volume PV 实体滚动条

实际的 partition（或 DISK）需要调整系统标识符（system ID）成为 8e（LVM 的标识符），再经过 pvcreate 的指令将他转成 LVM 最底层的实体滚动条（LV），之后才可以将这些 PV 加以利用，调整 system ID 的方式是通过 gdisk

### Volume Group VG 滚动条组

LVM 大磁盘是将许多 PV 整合成这个 VG ，所以 VG 是 LVM 组合起来的打磁盘。在预设情况下，使用 32 位的 Linux 系统时，基本上 LV 最大仅能支持到 65534 个 PE ，若使用预设的 PE 为 4MB 的情况下，最大容量可以达到约 256Gb，而在 64 位的 Linux 上，LV 几乎没有什么容量限制了

### Physical Extent PE 实体范围区块

LVM 预设使用 4MB 的 PE 区块，LV 在 32 位系统上最多有 65534 个 PE（lvml 的格式），因此预设的 LVM 的 LV 有 4M*65534/1024M=256G.

PE 是整个 LVM 最小的存储块，我们写入的数据都是写入 PE 的，简单说，PE 类似系统里面的 block。

所以调整 PE 会影响到 LVM 的最大容量，不过在 CentOS 6.x 以后，直接使用 lvm2 的各项格式功能，以及系统变成了 64 位，因此整个限制以及不存在了

### Logical Volume LV 逻辑滚动条

最终的 VG 还会被被切成 LV，LV 是最后可以被格式化使用的类似分区槽的东西，由于 PE 是整个 LVM 的最小存储单位，那么 LV 的大小就与 LV 内的 PE 总数有关。为了方便用户利用 LVM 来管理其系统，因此 LV 的装置文件名通常指定为 `/dev/vgname/lvname`的样式

LVM 可弹性的变更 filesystem 的容量是通过「交换 PE」来进行数据转换，将原本 LV 内的 PE 转移到其他装置中以降低 LV 容量，或将其他装置的 PE 加到此 LV 中以加大容量。VG、LV 与 PE 的关系类似下图：

![image-20200301205307039](http://p4ui.toweydoc.tech:20080/images/stydocs/image-20200301205307039.png)

如上图，VG 内的 PE 会分给虚线部分的 LV，如果未来这个 VG 要扩充的话，加上其他的 PV 即可。而最重要的 LV 如果要扩充的话，也是通过加入 VG 内没有使用到的 PE 来扩充的

### 实操流程

通过 PV、VG、LV 的规划之后，再利用 mkfs 将 LV 格式化为可以利用的文件系统，而这个文件系统的容量在未来还可以扩充或减少，而且里面的数据还不会被影响，整个流程由基础到最终的结果可以这样看：

![image-20200301205826020](http://p4ui.toweydoc.tech:20080/images/stydocs/image-20200301205826020.png)

当数据写入 LV 时，有两种机制写入到硬盘中：

- 线性模式（linear）

  假如将 `/dev/vda1、/dev/vdb1` 这两个 parition 假如到 VG 中，并且整个 VG 只有一个 LV 时，那么线性模式就是：当 `/dev/vda1` 的容量用完之后，`/dev/vdb1`的硬盘才会被使用到，这也是我们建议的模式

- 交错模式（triped）

  就好理解了，一份数据分割在多个硬盘上，理论上读写效率会好点

基本上，**LVM 最主要的用途是实现一个可以弹性调整容量的文件系统上**，而不是建立一个有效率为主的磁盘；因为数据分割在多个硬盘上，只要坏掉一块硬盘，那么基本上会损失掉很多数据。不适合 LVM

##  LVM 实验流程

LVM 必须核心有支持且需要安装 lvm2 软件，较新的 distribution 已经预设 LVM 相关软件安装了。

上一节实验 RAID 有 5 个分区，这里还是建议修改下 systemID 比较好，将 RAID 的 fd 改为 LVM 的 8e，实验流程如下：

- 使用 4 个 partition，每个 partition 容量为 1GB，且 system ID 需要为 8e
- 全部的 partition 整合为一个 VG，VG 名称设置为 vmrcode；且 PE 的大小为 16MB
- 建立一个名为 vmrcode 的 LV，容量大约 2G
- 最终这个 LV 格式化 xfs 的文件系统且挂载 `/srv/lvm` 中

0. disk 阶段，实际的磁盘

  这里需要准备把之前的 5 个分区删掉，重新格式化出来；有关分区相关操作请参考[第 7 章的硬盘分区、格式化章节](../07/03/)
  
  ```bash
  # 最终分区出来的信息如下
  Number  Start (sector)    End (sector)  Size       Code  Name
     1            2048            6143   2.0 MiB     EF02  
     2            6144         2103295   1024.0 MiB  0700  
     3         2103296        65026047   30.0 GiB    8E00  
     4        65026048        67123199   1024.0 MiB  8E00  Linux LVM
     5        67123200        69220351   1024.0 MiB  8E00  Linux LVM
     6        69220352        71317503   1024.0 MiB  8E00  Linux LVM
   7        71317504        73414655   1024.0 MiB  8E00  Linux LVM
     8        73414656        75511807   1024.0 MiB  8E00  Linux LVM
  
  # 前面说的 8e 就是这里的 Code 字段，是在建立分区的时候指定的 8E00,也就是 LVM 格式
  ```

1. PV 阶段

   下面是与 PV 有关的指令

   - pvcreate：将实体 partition 建立成为 PV
   - pvscan：搜索目前系统里面任何具有 PV 的磁盘
   - pvdisplay：显示目前系统上 PV 状态
   - pvremove：将 PV 属性移除，让该 partition 不具有 PV 属性

   ```bash
   # 检查有无 PV 在系统上
   [root@study ~]# pvscan 
     PV /dev/sda3   VG centos          lvm2 [30.00 GiB / 14.00 GiB free]
     Total: 1 [30.00 GiB] / in use: 1 [30.00 GiB] / in no VG: 0 [0   ]
   # 在安装的时候就使用了 LVM 了， /dev/sda3 就是
   
   # 将 /dev/sda{4-7} 建立成为 pv 格式
   [root@study ~]# pvcreate /dev/sda{4,5,6,7}
     Physical volume "/dev/sda4" successfully created.
     Physical volume "/dev/sda5" successfully created.
     Physical volume "/dev/sda6" successfully created.
     Physical volume "/dev/sda7" successfully created.
   [root@study ~]# pvscan 
     PV /dev/sda3   VG centos          lvm2 [30.00 GiB / 14.00 GiB free]
     PV /dev/sda6                      lvm2 [1.00 GiB]
     PV /dev/sda4                      lvm2 [1.00 GiB]
     PV /dev/sda5                      lvm2 [1.00 GiB]
     PV /dev/sda7                      lvm2 [1.00 GiB]
     Total: 5 [34.00 GiB] / in use: 1 [30.00 GiB] / in no VG: 4 [4.00 GiB]
    # 最后一行显示整体 PV 的量 / 已经被使用到的 VG 的 PV 量 / 剩余 PV 量
     
    # 更详细的列出系统上每个 PV 的个别信息
    [root@study ~]# pvdisplay /dev/sda4
     "/dev/sda4" is a new physical volume of "1.00 GiB"
     --- NEW Physical volume ---
     PV Name               /dev/sda4		# 实际的 partition 装置名称
     VG Name               				  # 因为尚未分配出去，所以空白的
     PV Size               1.00 GiB		# 容量
     Allocatable           NO				# 是否已被分配
     PE Size               0   			 # 此 PV 内 PE 的大小
     Total PE              0				  # 共分区出几个 PE
     Free PE               0				  # 没被 LV 用掉的 PE
     Allocated PE          0				  # 可以被分配出去的 PE 数量	
     PV UUID               ecfFVZ-Qesj-6xhF-3p0w-9GCO-ULGG-q9Kpg9
     # 由于 PE 是建立 VG 时才给予的参数，所以这里看到的 PE 都会是 0
     # 而且也没有多余的 PE 可共分配（allocatable）
   ```

2. VG 阶段

   与 VG 相关指令有：

   - vgcreate：主要建立 VG 的指令，参数较多
   - vgscan：搜索系统上的 VG
   - vgdisplay：显示 VG 状态
   - vgextend：在 VG 内增加额外的 PV
   - vgreduce：在 VG 内移除 PV
   - vgchange：设置 VG 是否启动（active）
   - vgremove：删除一个 VG

   PV 的名称其实就是 partition 的装置文件名，而 VG 的名称是自定义的。

   ```bash
   vgcreate [-s N[mgt]] VG名称 PV名称
   
   选项与参数：
   	-s：后面接 PE 的大小 site，单位可以是 m、g、t 大小写均可
   ```

   ```bash
   # 将 /dev/sda4-6 建立为一个 VG，且指定 PE 为 16MB
   [root@study ~]# vgcreate -s 16M vmcrcodevg /dev/sda{4,5,6}
     Volume group "vmcrcodevg" successfully created
   [root@study ~]# pvscan 
     PV /dev/sda4   VG vmcrcodevg      lvm2 [1008.00 MiB / 1008.00 MiB free]
     PV /dev/sda5   VG vmcrcodevg      lvm2 [1008.00 MiB / 1008.00 MiB free]
     PV /dev/sda6   VG vmcrcodevg      lvm2 [1008.00 MiB / 1008.00 MiB free]
     PV /dev/sda3   VG centos          lvm2 [30.00 GiB / 14.00 GiB free]
     PV /dev/sda7                      lvm2 [1.00 GiB]
     Total: 5 [33.95 GiB] / in use: 4 [32.95 GiB] / in no VG: 1 [1.00 GiB]
   # 可以发现有 3 个 PV被分掉了
   
   [root@study ~]# vgdisplay vmcrcodevg
     --- Volume group ---
     VG Name               vmcrcodevg
     System ID             
     Format                lvm2
     Metadata Areas        3
     Metadata Sequence No  1
     VG Access             read/write
     VG Status             resizable
     MAX LV                0
     Cur LV                0
     Open LV               0
     Max PV                0
     Cur PV                3
     Act PV                3
     VG Size               2.95 GiB		# 整体 VG 容量大小
     PE Size               16.00 MiB		# 内部每个 PE 的大小
     Total PE              189				# 总共 PE 的数量
     Alloc PE / Size       0 / 0   
     Free  PE / Size       189 / 2.95 GiB		# 可配置给 LV 的 PE 数量/ 总容量
     VG UUID               JQP2VZ-ilbN-MBuw-jccl-do84-tHu6-BInUXS
   
   # 剩余的 PV /dev/sda7 分配个 给 vmcrcodevg
   # 在 VG 内增加额外的 PV
   [root@study ~]# vgextend vmcrcodevg /dev/sda7
     Volume group "vmcrcodevg" successfully extended
   [root@study ~]# vgdisplay vmcrcodevg
     --- Volume group ---
     VG Name               vmcrcodevg
     System ID             
     Format                lvm2
     Metadata Areas        4
     Metadata Sequence No  2
     VG Access             read/write
     VG Status             resizable
     MAX LV                0
     Cur LV                0
     Open LV               0
     Max PV                0
     Cur PV                4
     Act PV                4
     VG Size               <3.94 GiB
     PE Size               16.00 MiB
     Total PE              252				# PE 变大了，可用容量也变大了
     Alloc PE / Size       0 / 0   
     Free  PE / Size       252 / <3.94 GiB
     VG UUID               JQP2VZ-ilbN-MBuw-jccl-do84-tHu6-BInUXS
   
   ```

3. LV 阶段

   有了 VG 这个大磁盘之后，就可以建立分区了。先了解与 LV 相关的指令：

   - lvcreate：建立 LV
   - lvscan：查询系统上的 LV
   - lvdisplay：显示系统上面的 LV 状态
   - lvextend：在 LV 里面增加容量
   - lvreduce：在 LV 里面减少容量
   - lvremove：删除一个 LV
   - lvresize：对 LV 进行容量大小调整

   ```bash
   lvcreate [-L N[mgt]] [-n lv名称] VG名称
   lvcreate [-l n] [-n lv名称] VG名称
   
   选项与参数：
   	-L：后面接容量，要注意：最小单位为 PE，也就是必须的 PE 的倍数，如果不是，系统则自行计算最相近的容量
   	-l：后面接 PE 的个数，若要这么做，需要自行计算 PE 数
   	-n：后面接 LV 的名称
   ```

   ```bash
   # 将 vmcrcodevg 分 2GB 给 vmmrcodelv
   [root@study ~]# lvcreate -L 2G -n vmmrcodelv vmcrcodevg
     Logical volume "vmmrcodelv" created.
   [root@study ~]# lvscan 
     ACTIVE            '/dev/vmcrcodevg/vmmrcodelv' [2.00 GiB] inherit
     ACTIVE            '/dev/centos/root' [10.00 GiB] inherit
     ACTIVE            '/dev/centos/home' [5.00 GiB] inherit
     ACTIVE            '/dev/centos/swap' [1.00 GiB] inherit
   # 由于本列中每个 PE 为 16M，如果要用 PE 的数量来处理的话，可以用下面的指令
   # lvcreate -l 128 -n vmmrcodelv vmcrcodevg
   
   [root@study ~]# lvdisplay /dev/vmcrcodevg/vmmrcodelv 
     --- Logical volume ---
     LV Path                /dev/vmcrcodevg/vmmrcodelv		# lv 全名
     LV Name                vmmrcodelv
     VG Name                vmcrcodevg
     LV UUID                V0WB3Y-SQr3-4hLb-D5IC-8fN6-6PEY-NPdJxz
     LV Write Access        read/write
     LV Creation host, time study.centos.mrcode, 2020-03-01 22:03:40 +0800
     LV Status              available
     # open                 0
     LV Size                2.00 GiB		# 容量
     Current LE             128
     Segments               3
     Allocation             inherit
     Read ahead sectors     auto
     - currently set to     8192
     Block device           253:3
   
   ```

4. 文件系统阶段

   ```bash
   [root@study ~]# mkfs.xfs /dev/vmcrcodevg/vmmrcodelv 
   meta-data=/dev/vmcrcodevg/vmmrcodelv isize=512    agcount=4, agsize=131072 blks
            =                       sectsz=512   attr=2, projid32bit=1
            =                       crc=1        finobt=0, sparse=0
   data     =                       bsize=4096   blocks=524288, imaxpct=25
            =                       sunit=0      swidth=0 blks
   naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
   log      =internal log           bsize=4096   blocks=2560, version=2
            =                       sectsz=512   sunit=0 blks, lazy-count=1
   realtime =none                   extsz=4096   blocks=0, rtextents=0
   [root@study ~]# mount /dev/vmcrcodevg/vmmrcodelv /srv/lvm/
   [root@study ~]# df -Th /srv/lvm/
   文件系统                          类型  容量  已用  可用 已用% 挂载点
   /dev/mapper/vmcrcodevg-vmmrcodelv xfs   2.0G   33M  2.0G    2% /srv/lvm
   
   ```

   这就建立好一个 LV 了，可以使用  /srv/lvm 了

## 放大 LV 容量

LVM 最大的特色是弹性调整磁盘容量，前面说到，放大 LV 需要有下面这些流程：

1. VG 阶段需要有剩余的容量

   如果 VG 阶段没有多余的容量，那么就增加硬盘等手段创建 PV，然后增加到 VG 中，可利用 pvcreate 和 vgextedn 来操作

2. LV 阶段产生更多可用容量

   有了足够的 VG，那么可以使用 lvresize 指令将剩余容量加入到 LV 中

3. 文件系统阶段放大

   Linux 实际使用的其实不是 LV，而是 LV 这个装置内的文件系统，所以一切最终还是以文件系统为依归。目前在 Linux 下，作者测试过可以放大的文件系统有 XFS 和 EXT 家族，至于缩小则只有 EXT 家族，目前 XFS 文件系统不支持文件系统的容量缩小

   xfs 放大可以通过 xfs_growfs 指令

最后一个步骤最重要，在第  7 章中知道，整个文件系统在最初格式化的时候就建立了 inode、block、superblock 等信息，要改变这些信息很难，不过因为文件系统格式化的时候建立的是多个 block group，因此可以通过在文件系统中增加 block group 的方式来增减文件系统的量，而增加 block group 就是利用 xfs_growfs 指令

另外，严格说起来，放大文件系统并不是没有进行格式化，格式化的位置在于该装置后来新增的部分，装置的前面已经存在的文件系统没有任何变化。新增的格式化过的数据，再反馈回原本的 supberblock 。

这里针对 `/srv/lvm` 增加 500MB 容量

```bash
# 检查 VG
[root@study ~]# vgdisplay vmcrcodevg 
  --- Volume group ---
  VG Name               vmcrcodevg
  System ID             
  Format                lvm2
  Metadata Areas        4
  Metadata Sequence No  3
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                1
  Open LV               1
  Max PV                0
  Cur PV                4
  Act PV                4
  VG Size               <3.94 GiB
  PE Size               16.00 MiB
  Total PE              252
  Alloc PE / Size       128 / 2.00 GiB
  Free  PE / Size       124 / <1.94 GiB	# 这里还有剩余的 PE 未使用
  VG UUID               JQP2VZ-ilbN-MBuw-jccl-do84-tHu6-BInUXS
  
# 直接放大 LV
[root@study ~]# lvresize -L +500 /dev/vmcrcodevg/vmmrcodelv 
  Rounding size to boundary between physical extents: 512.00 MiB.
  Size of logical volume vmcrcodevg/vmmrcodelv changed from 2.00 GiB (128 extents) to 2.50 GiB (160 extents).
  Logical volume vmcrcodevg/vmmrcodelv successfully resized.
[root@study ~]# lvscan 
  ACTIVE            '/dev/vmcrcodevg/vmmrcodelv' [2.50 GiB] inherit
  ACTIVE            '/dev/centos/root' [10.00 GiB] inherit
  ACTIVE            '/dev/centos/home' [5.00 GiB] inherit
  ACTIVE            '/dev/centos/swap' [1.00 GiB] inherit
# 可以看到 lv 变大了

[root@study ~]# df -Th /srv/lvm/
Filesystem                        Type  Size  Used Avail Use% Mounted on
/dev/mapper/vmcrcodevg-vmmrcodelv xfs   2.0G   33M  2.0G   2% /srv/lvm
```

可以看到 LV 增加到了 2.5G，但是文件系统还未增加。下面继续处理文件系统的扩容

```bash
# 查看原文件系统内的 superblock 记录
[root@study ~]# xfs_info /srv/lvm/
meta-data=/dev/mapper/vmcrcodevg-vmmrcodelv isize=512    agcount=4, agsize=131072 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=0 spinodes=0
data     =                       bsize=4096   blocks=524288, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
log      =internal               bsize=4096   blocks=2560, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0
# agcount=4，blocks=524288

# 很关键的一步来了
[root@study ~]# xfs_growfs /srv/lvm/
meta-data=/dev/mapper/vmcrcodevg-vmmrcodelv isize=512    agcount=4, agsize=131072 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=0 spinodes=0
data     =                       bsize=4096   blocks=524288, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
log      =internal               bsize=4096   blocks=2560, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0
data blocks changed from 524288 to 655360
# 这里的 blocks 改变到 655360 了
[root@study ~]# xfs_info /srv/lvm/
meta-data=/dev/mapper/vmcrcodevg-vmmrcodelv isize=512    agcount=5, agsize=131072 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=0 spinodes=0
data     =                       bsize=4096   blocks=655360, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
log      =internal               bsize=4096   blocks=2560, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0
# 现在 agcount=5，blocks=655360 了
[root@study ~]# df -Th  /srv/lvm/
Filesystem                        Type  Size  Used Avail Use% Mounted on
/dev/mapper/vmcrcodevg-vmmrcodelv xfs   2.5G   33M  2.5G   2% /srv/lvm

```

如上展示了，在不停机的情况下可以进行扩容，很棒的特性；

最后注意，目前的 XFS 文件系统中，没有缩小文件系统容量的设计。只有 EXT 家族可以

## 使用 LVM thin Volume 让 LVM 动态自动调整磁盘使用率

考虑这样一个场景：有个目录未来会使用到大约 5T 的容量，但是目前你的磁盘只有 3T，但接下来的两个月你的系统都不会超过 3T 的容量，但是你要让用户知道，他最最多有 5T 的空间可以使用，而且在一个月内你确实可以将系统提升到 5T 以上的容量，又不想在提升容量后才放大到 5T，怎么办？

这个时候可以考虑：实际用多少才分配多少容量给 LV 的 LVM Thin Volume 功能

再考虑一个环境：你需要 3 个 10GB 的硬盘进行某些测试，但是你的环境只有 5Gb 的剩余容量，在传统的 LVM 环境下，LV 的容量是一开始就分配好的，因此你没有办法在这样的环境中产生出 3 个 10GB 的装置，而且 10GB 的装置其实每个实际使用率都没有超过 10%，也就是总用量目前只用到 3GB，你实际有 5GB的容量，那么久可以使用 LVM Thin Volume 做出 3 个只用 1GB 的 10GB 装置

LVM thin Volume 的概念是：先建立一个可以实支实付、用多少容量才分配实际写入多少容量的磁盘容量存储池（thin pool），然后再由这个 thin pool 去产生一个「指定要固定容量大小的 LV 装置」，但是这个 LV 在声明上，他的容量可能有 10GB，但实际上，该装置用到多少容量时，才会从 thin pool 去实际取得所需的容量』

thin pool 只有 1GB 的实际容量，但是可以分配一个 10GB 的 LV 装置，该装置实际用到 500M 时，thin pool 也就只有 500M 分配给他，但是由 thin pool 分配出来的 LV 总实际使用量绝对不能超过 thin pool 的最大实际容量

简单可以理解为：实际上你只有 1G 物理容量，但是你分配一个 10G 的 lv，只是这个 10G 的 lv 最大实际容量也就只有 1G

下面使用剩余的容量，来实践练习：

1. 由 vmcrcodevg 的剩余容量取出 1GB 来做出一个名为 vmrcodetpool 的 thin pool LV 装置，这就是所谓的磁盘容量存储池 thin pool
2. 由 vmcrcodevg 内的 vmrcodetpool 产生一个名为 vmrcodethin1 的 10GB LV 装置
3. 将此装置实际格式化为 XFS 文件系统，并且挂载到 `/srv/thin` 目录内

```bash
# 创建 thin pool
[root@study ~]# lvcreate -L 1G -T vmcrcodevg/vmrcodetpool
  Thin pool volume with chunk size 64.00 KiB can address at most 15.81 TiB of data.
  Logical volume "vmrcodetpool" created.

[root@study ~]# lvdisplay /dev/vmcrcodevg/vmrcodetpool 
  --- Logical volume ---
  LV Name                vmrcodetpool
  VG Name                vmcrcodevg
  LV UUID                163HRB-1gOB-6SOS-Dion-g29K-uOZr-deGTyl
  LV Write Access        read/write
  LV Creation host, time study.centos.mrcode, 2020-03-01 22:40:42 +0800
  LV Pool metadata       vmrcodetpool_tmeta
  LV Pool data           vmrcodetpool_tdata
  LV Status              available
  # open                 0
  LV Size                1.00 GiB		# 总共可分配出去容量
  Allocated pool data    0.00%			# 以分配的容量百分比
  Allocated metadata     10.23%			# 以分配的数据百分比
  Current LE             64
  Segments               1
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     8192
  Block device           253:6

# 语法为 lvs VGname
[root@study ~]# lvs vmcrcodevg
  LV           VG         Attr       LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  vmmrcodelv   vmcrcodevg -wi-ao---- 2.50g                                                    
  vmrcodetpool vmcrcodevg twi-a-tz-- 1.00g             0.00   10.23 
  
# 开始建立 10GB 的装置
[root@study ~]# lvcreate -V 10G -T vmcrcodevg/vmrcodetpool -n vmrcrodethin1
  WARNING: Sum of all thin volume sizes (10.00 GiB) exceeds the size of thin pool vmcrcodevg/vmrcodetpool and the size of whole volume group (<3.94 GiB).
  WARNING: You have not turned on protection against thin pools running out of space.
  WARNING: Set activation/thin_pool_autoextend_threshold below 100 to trigger automatic extension of thin pools before they get full.
  Logical volume "vmrcrodethin1" created.
  
[root@study ~]# lvs vmcrcodevg
  LV            VG         Attr       LSize  Pool         Origin Data%  Meta%  Move Log Cpy%Sync Convert
  vmmrcodelv    vmcrcodevg -wi-ao----  2.50g                                                            
  vmrcodetpool  vmcrcodevg twi-aotz--  1.00g                     0.00   10.25                           
  vmrcrodethin1 vmcrcodevg Vwi-a-tz-- 10.00g vmrcodetpool        0.00 
# 多了一个 vmrcrodethin1 还使用了 Pool，并且是 10Gb 的

# 创建文件系统
[root@study ~]# mkfs.xfs /dev/vmcrcodevg/vmrcrodethin1 
meta-data=/dev/vmcrcodevg/vmrcrodethin1 isize=512    agcount=16, agsize=163840 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=0, sparse=0
data     =                       bsize=4096   blocks=2621440, imaxpct=25
         =                       sunit=16     swidth=16 blks
naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
log      =internal log           bsize=4096   blocks=2560, version=2
         =                       sectsz=512   sunit=16 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0
[root@study ~]# mkdir /srv/thin
[root@study ~]# mount /dev/vmcrcodevg/vmrcrodethin1 /srv/thin/
[root@study ~]# df -Th /srv/thin/
Filesystem                           Type  Size  Used Avail Use% Mounted on
/dev/mapper/vmcrcodevg-vmrcrodethin1 xfs    10G   33M   10G   1% /srv/thin
# 可以看到文件系统中显示有 10GB 的容量

# 测试下容量的使用，建立 500MB 的文件
[root@study ~]# dd if=/dev/zero of=/srv/thin/test.img bs=1M count=500
500+0 records in
500+0 records out
524288000 bytes (524 MB) copied, 2.17685 s, 241 MB/s
[root@study ~]# df -Th /srv/thin/
Filesystem                           Type  Size  Used Avail Use% Mounted on
/dev/mapper/vmcrcodevg-vmrcrodethin1 xfs    10G  533M  9.5G   6% /srv/thin

[root@study ~]# lvs vmcrcodevg
  LV            VG         Attr       LSize  Pool         Origin Data%  Meta%  Move Log Cpy%Sync Convert
  vmmrcodelv    vmcrcodevg -wi-ao----  2.50g                                                            
  vmrcodetpool  vmcrcodevg twi-aotz--  1.00g                     49.92  11.82                           
  vmrcrodethin1 vmcrcodevg Vwi-aotz-- 10.00g vmrcodetpool        4.99 
# 这时已经分配出去 49.92 的容量了，而 vmrcrodethin1 却只看到用掉了 4.99% 而已
```

所以 thin pool 非常好用，但是在管理上，要特别的留意

基本上 thin pool ，可以用来骗人，一个磁盘可以仿真出很多容量来，实际可用容量只有物理磁盘的容量，如果超过该容量，这个 thin pool 中的资料会损坏

## LVM 的 LV 磁盘快照

LVM 还有一个重要的能力，就是磁盘快照：将当时的系统信息记录下来，就好像照相记录一般，未来若有任何资料变动了，则原始资料会被搬移到快照区，没有被变动的区域则由快照区与文件系统共享。

![image-20200302221604971](http://p4ui.toweydoc.tech:20080/images/stydocs/image-20200302221604971.png)   

上图很形象了，建立 LVM 会预留一个区域（左图的三个 PE 块）作为快照数据的存放。

由于快照区与原本的 LV 共享很多 PE 区块，因此 **快照区与被快照的 LV 必须要在同一个 VG 上**

另外，不建议使用 thin pool 来制作快照，限制太多了；

下面针对传统的 LV 磁盘进行快照的配置，大致流程为：

- 预计被拿来备份的原始 LV 为 `/dev/vmcrcodevg/vmmrcodelv`
- 使用传统方式快照建立，原始磁盘为 `/dev/vmcrcodevg/vmmrcodelv`，快照名称为 vmrcodesnap1，容量为 vmcrcodevg 的所有剩余容量

### 传统快照区的建立

```bash
# 观察 VG 剩余容量
[root@study vmcrcodevg]# vgdisplay vmcrcodevg 
  --- Volume group ---
  VG Name               vmcrcodevg
  System ID             
  Format                lvm2
  Metadata Areas        4
  Metadata Sequence No  9
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                3
  Open LV               0
  Max PV                0
  Cur PV                4
  Act PV                4
  VG Size               <3.94 GiB
  PE Size               16.00 MiB
  Total PE              252
  Alloc PE / Size       226 / 3.53 GiB
  Free  PE / Size       26 / 416.00 MiB			# 还有 26 个PE
  VG UUID               JQP2VZ-ilbN-MBuw-jccl-do84-tHu6-BInUXS
  
  
 # 创建快照区
 # 注意：-n 后面是给快照区取名，笔者这里看错了，用了 vg 的名称
 # 本应该与书上的 vmrcodesnap1 作为快照名称的
 [root@study vmcrcodevg]# lvcreate -s -l 26 -n vmcrcodevg /dev/vmcrcodevg/vmmrcodelv 
  WARNING: Sum of all thin volume sizes (10.00 GiB) exceeds the size of thin pools and the size of whole volume group (<3.94 GiB).
  WARNING: You have not turned on protection against thin pools running out of space.
  WARNING: Set activation/thin_pool_autoextend_threshold below 100 to trigger automatic extension of thin pools before they get full.
  Logical volume "vmcrcodevg" created.
# -s 表示  snapshot 功能， -n 后面则是快照区名称，再后面是在哪个 lv 上创建快照
# -l 后面则表示使用多少个 PE 来作为整个快照区使用

# 不过还好，使用了 vg
[root@study vmcrcodevg]# lvdisplay /dev/vmcrcodevg/vmcrcodevg 
  --- Logical volume ---
  LV Path                /dev/vmcrcodevg/vmcrcodevg
  LV Name                vmcrcodevg
  VG Name                vmcrcodevg
  LV UUID                gprMsU-6fXb-mAhu-xehC-jksk-rj4M-0na11O
  LV Write Access        read/write
  LV Creation host, time study.centos.mrcode, 2020-03-02 22:26:19 +0800
  LV snapshot status     active destination for vmmrcodelv
  LV Status              available
  # open                 0
  LV Size                2.50 GiB				# 原始碟  lv 的容量
  Current LE             160
  COW-table size         416.00 MiB			# 这个快照能够记录的最大容量
  COW-table LE           26
  Allocated to snapshot  0.00%					# 目前已被使用掉的容量
  Snapshot chunk size    4.00 KiB
  Segments               1
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     8192
  Block device           253:11

```

快照区被建立起来了，仔细看他的 VG Name 与原本的 LV 所在 VG 相同。

下面来挂载这个快照区，挂载之后，你会发现里面的数据与原来的 LV 的数据是一样的

```bash
[root@study vmcrcodevg]# mkdir /srv/snapshot1
[root@study vmcrcodevg]# mount -o nouuid /dev/vmcrcodevg/vmcrcodevg /srv/snapshot1/
[root@study vmcrcodevg]# df -Th /srv/lvm/ /srv/snapshot1/
Filesystem                        Type  Size  Used Avail Use% Mounted on
/dev/mapper/vmcrcodevg-vmmrcodelv xfs   2.5G   33M  2.5G   2% /srv/lvm
/dev/mapper/vmcrcodevg-vmcrcodevg xfs   2.5G   33M  2.5G   2% /srv/snapshot1

# 可以看到数据是一模一样的，我们都没有动过该数据
# 快照区会主动记录员 lv 的内容
# nouui 参数：XFS 不允许相同的 UUID 文件系统挂载，所以需要加上让它忽略相同的 UUID 所造成的问题
# 因为快照出来的文件系统是一模一样的
```

### 利用快照区复原系统

需要注意的是：要复原的数据量不能高于快照区所能负载的实际容量。否则快照区肯定装不下，就会失效

```bash
# 1. 增加一些内容到 lvm ，查看两个区块的变化
[root@study vmcrcodevg]# df -Th /srv/lvm/ /srv/snapshot1/
Filesystem                        Type  Size  Used Avail Use% Mounted on
/dev/mapper/vmcrcodevg-vmmrcodelv xfs   2.5G   33M  2.5G   2% /srv/lvm
/dev/mapper/vmcrcodevg-vmcrcodevg xfs   2.5G   33M  2.5G   2% /srv/snapshot1

# 由于之前我忘记拷贝一些数据到 lvm 下了，看目录下面是空的内容
# 这里直接 copy 数据到该目录下
[root@study vmcrcodevg]# cp -a /etc/ /srv/lvm/
[root@study vmcrcodevg]# df -Th /srv/lvm/ /srv/snapshot1/
Filesystem                        Type  Size  Used Avail Use% Mounted on
/dev/mapper/vmcrcodevg-vmmrcodelv xfs   2.5G   92M  2.5G   4% /srv/lvm
/dev/mapper/vmcrcodevg-vmcrcodevg xfs   2.5G   33M  2.5G   2% /srv/snapshot1

[root@study vmcrcodevg]# ll /srv/lvm /srv/snapshot1/
/srv/lvm:
total 12
drwxr-xr-x. 143 root root 8192 Mar  1 17:29 etc

/srv/snapshot1/:
total 0

# 现在两个目录不一样了，检测一下快照 LV
[root@study vmcrcodevg]# lvdisplay /dev/vmcrcodevg/vmcrcodevg 
  --- Logical volume ---
  LV Path                /dev/vmcrcodevg/vmcrcodevg
  LV Name                vmcrcodevg
  VG Name                vmcrcodevg
  LV UUID                gprMsU-6fXb-mAhu-xehC-jksk-rj4M-0na11O
  LV Write Access        read/write
  LV Creation host, time study.centos.mrcode, 2020-03-02 22:26:19 +0800
  LV snapshot status     active destination for vmmrcodelv
  LV Status              available
  # open                 1
  LV Size                2.50 GiB
  Current LE             160
  COW-table size         416.00 MiB
  COW-table LE           26
  Allocated to snapshot  11.05%		# 目前被使用掉的容量
  Snapshot chunk size    4.00 KiB
  Segments               1
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     8192
  Block device           253:11

# 2. 利用快照区将原本的 filesystem 备份，使用 xfsdump 来处理
[root@study vmcrcodevg]# xfsdump -l 0 -L lvm1 -M lvm1 -f /home/lvm.dump /srv/snapshot1 
xfsdump: using file dump (drive_simple) strategy
xfsdump: version 3.1.7 (dump format 3.0) - type ^C for status and control
xfsdump: level 0 dump of study.centos.mrcode:/srv/snapshot1
xfsdump: dump date: Mon Mar  2 22:48:52 2020
xfsdump: session id: cf94cc10-3823-4138-b016-cc7e4c3b8964
xfsdump: session label: "lvm1"
xfsdump: ino map phase 1: constructing initial dump list
xfsdump: ino map phase 2: skipping (no pruning necessary)
xfsdump: ino map phase 3: skipping (only one dump stream)
xfsdump: ino map construction complete
xfsdump: estimated dump size: 20800 bytes
xfsdump: creating dump session media file 0 (media 0, file 0)
xfsdump: dumping ino map
xfsdump: dumping directories
xfsdump: dumping non-directory files
xfsdump: ending media file
xfsdump: media file size 21016 bytes
xfsdump: dump size (non-dir files) : 0 bytes
xfsdump: dump complete: 0 seconds elapsed
xfsdump: Dump Summary:
xfsdump:   stream 0 /home/lvm.dump OK (success)
xfsdump: Dump Status: SUCCESS

```

为什么要备份？为什么不直接格式化？前面说到过，快照只会存储变化的数据，如果格式化，那么所有数据都会存到快照区的。快照区那么小，一般都不够存储的

```bash
# 3. 将 /srv/snapshot1 卸载并移除（因为里面的内容以及备份起来了）
[root@study vmcrcodevg]# umount /srv/snapshot1/
[root@study vmcrcodevg]# lvremove /dev/vmcrcodevg/vmcrcodevg 
Do you really want to remove active logical volume vmcrcodevg/vmcrcodevg? [y/n]: y
  Logical volume "vmcrcodevg" successfully removed
# 把快照功能卸除之后，把原始盘格式化
[root@study vmcrcodevg]# mkfs.xfs -f /dev/vmcrcodevg/vmmrcodelv 
meta-data=/dev/vmcrcodevg/vmmrcodelv isize=512    agcount=4, agsize=163840 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=0, sparse=0
data     =                       bsize=4096   blocks=655360, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
log      =internal log           bsize=4096   blocks=2560, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0
# 重新挂载，相当于已经没有数据了，
[root@study vmcrcodevg]# mount /dev/vmcrcodevg/vmmrcodelv /srv/lvm/
# 这里从备份的快照里面把数据恢复回去
[root@study vmcrcodevg]# xfsrestore -f /home/lvm.dump -L lvm1 /srv/lvm/
xfsrestore: using file dump (drive_simple) strategy
xfsrestore: version 3.1.7 (dump format 3.0) - type ^C for status and control
xfsrestore: using online session inventory
xfsrestore: searching media for directory dump
xfsrestore: examining media file 0
xfsrestore: reading directories
xfsrestore: 1 directories and 0 entries processed
xfsrestore: directory post-processing
xfsrestore: restore complete: 0 seconds elapsed
xfsrestore: Restore Summary:
xfsrestore:   stream 0 /home/lvm.dump OK (success)
xfsrestore: Restore Status: SUCCESS
[root@study vmcrcodevg]# ll /srv/lvm
total 0

# 这里变成了 0，从这里我大概明白了这个快照的流程了
# 1. 创建快照的时候，可能是把当时那一刻的所有数据都放到了快照区，由于实际数据小于快照区，所以可以放下
# 2. 对原始盘修改的时候，只要不创建快照，数据就不会被同步过去
# 3. 这里备份快照区，再从快照区恢复创建快照时的数据
```

利用快照区进行各项练习与测试的任务，再以原系统还原快照：大概意思就是说，将原始数据当成备份，快照出来的快照区进行测试（当成真正的目录一样使用），要还原的时候就直接删掉快照区，再创建一个快照即可。

这里的思路和我上面自己理解的貌似类似，但是下面这张图貌似就说不通了

![image-20200302221604971](http://p4ui.toweydoc.tech:20080/images/stydocs/image-20200302221604971.png)

因为上面我们快照了数据，会发现快照和原始数据是一样的，在修改原始数据后，快照区数据并诶呦变化，然后我们备份了快照区的数据，并删除了快照区；然后再格式化了原始碟，那么这里公用的 PE 是不是貌似就说不通了? 或则换个说法来说，快照区备份实际上是把创建快照那一刻的所有数据都备份了（就是说是所有的原始数据，因为在创建快照哪一个点的所有数据），但其实如果不用备份的话，是没法恢复快照的，因为这里公用了 PE 的问题；快照会区分变化的文件，备份的时候回排除掉变化的文件。

上面的解释貌似可以通了

## LVM 相关指令汇总与 LVM 的关闭

任务 | PV 阶段 | VG 阶段 | LV 阶段 | filesystem-xfs | filesystem-ext4 
:-:|:-:|:-:|:-:|:-:|:-:
搜索`scan`|pvscan|vgscan|lvscan|lsblk、blkid|lsblk、blkid
创建`create`|pvcreate|vgcreate|lvcreate|mkfs.xfs|mkfs.ext4
列出`display`|pvdisplay|vgdisplay|lvdisplay|df、mount|df、mount
增加`extend`||vgextend|lvextend(lvresize)|xfs_growfs|resize2fs
减少`reduce`||vgreduce|lvreduce(lvresize)|不支持|resize2fs
删除`remove`|pvremove|vgremove|lvremove|umount、重新格式化|umount、重新格式化
改变容量`resize`|||lvresize|xfs_grows|resize2fs
改变属性`attribute`|pvchange|vgchange|lvchange|/etc/fstab、remount|/etc/fstab、remount

文件系统阶段的格式化处理，需要以  xfs_growfs 来修正文件系统实际的大小。

会规划 LVM 还不行，还需要会移除 LVM，流程如下：

1. 先卸载系统上面的 LVM 文件系统（包括快照与所有 LV）
2. 使用 lvremove 移除 LV
3. 使用 `vgchange -a n VGname` 让 VGname 这个 VG 不具有 Active 标志
4. 使用 vgremove 移除 VG
5. 使用 pvremove 移除 PV
6. 使用 fsidk 修改 ID，改回来

下面进行卸载练习

```bash
[root@study ~]# umount /srv/lvm/ /srv/thin/ /srv/snapshot1/
[root@study ~]# lvs vmcrcodevg
  LV            VG         Attr       LSize  Pool         Origin Data%  Meta%  Move Log Cpy%Sync Convert
  vmmrcodelv    vmcrcodevg -wi-a-----  2.50g                                                            
  vmrcodetpool  vmcrcodevg twi-aotz--  1.00g                     49.92  11.82                           
  vmrcrodethin1 vmcrcodevg Vwi-a-tz-- 10.00g vmrcodetpool        4.99 
  
# 注意，删除顺序是：vmrcrodethin1 > vmrcrodethin1 > vmmrcodelv
[root@study ~]# lvremove /dev/vmcrcodevg/vmrcrodethin1  /dev/vmcrcodevg/vmrcodetpool 
Removing pool "vmrcodetpool" will remove 1 dependent volume(s). Proceed? [y/n]: y
Do you really want to remove active logical volume vmcrcodevg/vmrcrodethin1? [y/n]: y
  Logical volume "vmrcrodethin1" successfully removed
Do you really want to remove active logical volume vmcrcodevg/vmrcodetpool? [y/n]: y
  Logical volume "vmrcodetpool" successfully removed
[root@study ~]# lvremove /dev/vmcrcodevg/vmmrcodelv 
Do you really want to remove active logical volume vmcrcodevg/vmmrcodelv? [y/n]: y
  Logical volume "vmmrcodelv" successfully removed
  
[root@study ~]# vgchange -a n vmcrcodevg 
  0 logical volume(s) in volume group "vmcrcodevg" now active
  
[root@study ~]# vgremove vmcrcodevg 
  Volume group "vmcrcodevg" successfully removed

```

最后再使用 gdisk 将磁盘的 ID 给改回来 83 就可以了

```bash
gdisk /dev/sda
Command (? for help): p
Number  Start (sector)    End (sector)  Size       Code  Name
   1            2048            6143   2.0 MiB     EF02  
   2            6144         2103295   1024.0 MiB  0700  
   3         2103296        65026047   30.0 GiB    8E00  
   4        65026048        67123199   1024.0 MiB  8E00  Linux LVM
   5        67123200        69220351   1024.0 MiB  8E00  Linux LVM
   6        69220352        71317503   1024.0 MiB  8E00  Linux LVM
   7        71317504        73414655   1024.0 MiB  8E00  Linux LVM
   8        73414656        75511807   1024.0 MiB  8E00  Linux LVM
# 修改 ID 也就是这里的 Code，可以看到这几个分区还是 8E00
```

## 重点回顾

- Quota 可公平的分配系统上面的磁盘容量给用户；分配的资源可以是磁盘容量（block）或可建立文件数量
- Quota 的限制可以有 soft/hard/grace time 等重要
- Quota 是针对整个 filesystem 进行限制，XFS 文件系统可以限制目录
- Quota 的使用必须要核心与文件系统支持。文件系统的参数必须含有 usrquota、grpquota、prjquota
- Quota 的 xfs_quota 的指令有 report、print、limit、timer 等
- 磁盘阵列（RAID）有硬件与软件之分，Linux 操作系统可支持软件磁盘阵列，通过 mdadm 套件来达成
- 磁盘阵列建立考虑依据为容量、效率、资料可靠性等
- 磁盘阵列等级常见有 raid0、raid1、raid1+0、raid5、raid6
- 硬件磁盘阵列的装置文件名与 SCSI 相同，至于 software RAID 则为 `/dev/md[0-9]`
- 软件磁盘阵列的状态还可以通过 `/proc/mdstat` 文件来了解
- LVM 强调的是 弹性的变化文件系统的容量
- 与 LVM 有关的组件有 PV/VG/PE/LV 等组件，可以被格式化的有 LV
- 新的 LVM 拥有 LVM thin volume 功能，可以通过调整磁盘的使用率
- LVM 拥有快照功能，快照可以记录 LV 的数据内容，并与原有的 LV 共享未更改的数据，备份还原就变得很简单
- XFS 通过 xfs_grows 指令，可以弹性的调整文件系统的大小

## 参考数据与延伸阅读

- RAID 深入认识可参考：

  - http://www.ibiblio.org/pub/Linux/docs/HOWTO/other-formats/html_single/Software-RAID-0.4x-HOWTO.html
  - http://www.tldp.org/HOWTO/Software-RAID-HOWTO.html   此链接笔者打不开了

  关键词是 Software-RAID-HOWTO

- mdstat 说明可以参考：https://raid.wiki.kernel.org/index.php/Mdstat

- 徐秉义老师在网管人杂志的文章，分别是：磁盘管理：SoftRaid 与 LVM 中和实做应用（上/下）

  自己百度该文章